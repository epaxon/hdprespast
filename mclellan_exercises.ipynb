{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_examples(filepath):\n",
    "    examples = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                examples.append(word)\n",
    "    return examples\n",
    "\n",
    "# stage 1. just knowing\n",
    "simple_present = read_examples(\"simple_present.txt\")\n",
    "simple_past = read_examples(\"simple_past.txt\")\n",
    "\n",
    "# test set is now different\n",
    "\"\"\"\n",
    "test_simple_present = read_examples(\"test_simple_present.txt\")\n",
    "test_simple_past = read_examples(\"test_simple_past.txt\")\n",
    "\"\"\"\n",
    "\n",
    "# stage 2. predicting tense (regularized, less correct)\n",
    "\"\"\"\n",
    "http://www.myenglishteacher.net/irregular_verbs.html\n",
    "given these few examples, basically 1 example for every rule,\n",
    "generate the past tense for a test word\n",
    "test_tense.txt\n",
    "\"\"\"\n",
    "\n",
    "# stage 3. predicting tense, usually correct\n",
    "\"\"\"\n",
    "pg 8 mclellan\n",
    "feature vectors by one hot encoding.\n",
    "how to determine if a verb is irregular or regular. it seems like a specific mapping for irregular. \n",
    "other than -ed and tense endings, what kind of patterns are we using to estimate a verb to a tense?\n",
    "\n",
    "are we running linear regression where the elements of weight vector (every type of tense) determined by gaussians?\n",
    "what would an activation/threshold be?\n",
    "pg 9\n",
    "\n",
    "pg 11: sounds like we're gradient descending\n",
    "\n",
    "One hot encoding means can only store N linearly independent sets of patterns. can introduce noise from a gaussian \n",
    "hyperparameters mu = 0 and variance to have each set represent a general rule so can store more patterns. \n",
    "rule of 78 wat.\n",
    "pg 14\n",
    "\n",
    "enforce logarithmic growth for adding patterns of regular verbs\n",
    "pg 15\n",
    "\n",
    "A scheme which meets the first criterion, but not the second, is the scheme proposed by Wickelgren 0969) .\n",
    "He suggested that words\n",
    "should be represented as sequences of context-sensitive phoneme units\n",
    "which represent each phone in a word as a triple, consisting of the phone itself, its predecessor, and its successor.\n",
    "Notationally, we write each Wickelphone as a triple of phonemes, consisting of the central phoneme,\n",
    "subscripted on the left by its predecessor and on the right by its successor. \n",
    "A phoneme occur- ring at the beginning of a word is preceded by a special symbol (#) standing for the word boundary; \n",
    "pg 18\n",
    "\n",
    "Though the Wickelphones in a word are not strictly position specific , \n",
    "it turns out that (a) few words contain more than one occurrence of any given Wickelphone, \n",
    "and (b) there are no two words we know of that consist of the same sequence of Wickel- phones. \n",
    "For example Islitl and Isiltl contain no Wickelphones in common.\n",
    "One nice property of Wickelphones is that they capture enough of the context in which a phoneme \n",
    "occurs to provide a sufficient basis for differentiating between the different cases of the \n",
    "past-tense rule and for characterizing the contextual variables that determine the subregulari- ties \n",
    "among the irregular past-tense verbs. For example, the word-final phoneme that \n",
    "determines whether we should add Idl, It I or rdl forming the regular past. \n",
    "And it is the sequence iN # which transformed to aN # in the ing ang pattern found in words like sing.\n",
    "The trouble with the Wickelphone solution is that there are too many of them, and they are too specific\n",
    "pg 19\n",
    "\n",
    "for neural net:\n",
    "activation function: sigmoid, wickelphone, relu\n",
    "\n",
    "hat the model captures the basic three-stage pattern of acquisi- tion.\n",
    ". That the model captures most aspects of differences in per- formance on different types of regular and irregular verbs.\n",
    ". That the model is capable of responding appropriately to verbs it has never seen before , as well as to regular and irregular verbs actually experienced during training.\n",
    "The more frequent a verb is, the more likely it is to be a regular verb. \n",
    "pg 25\n",
    "\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
