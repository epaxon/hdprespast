{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import utils\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy import optimize\n",
    "from utils import random_idx\n",
    "from utils import utils\n",
    "from utils import lang_vectors_utils as lvu\n",
    "%matplotlib inline\n",
    "\n",
    "alphabet = string.lowercase + ' _#'\n",
    "k = 5000\n",
    "N = 10000\n",
    "\n",
    "cluster_sizes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "ordered = 1\n",
    "alphabet = string.lowercase + ' '\n",
    "RI_letters = random_idx.generate_letter_id_vectors(N, k, alphabet)\n",
    "lower_n_cutoff = .85\n",
    "\n",
    "def read_examples(filepath):\n",
    "    examples = set()\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                examples.add(word)\n",
    "    return examples\n",
    "\n",
    "def wicklefeaturize(past_tense_word, cluster_size, filepath=\"wickle_train/\"):\n",
    "    \"\"\"\n",
    "    Create a wicklefeature matrix (for mapping) and vector (for computations).\n",
    "    only trigrams (cluster_size of size 3).\n",
    "    Save the matrix.\n",
    "    \n",
    "    _ j u\n",
    "    j u m\n",
    "    u m p\n",
    "    m p #\n",
    "\n",
    "    \"\"\"\n",
    "    word = \"_\" + past_tense_word + \"#\"\n",
    "    wicklefeatures = np.zeros((len(word)-cluster_size,N))\n",
    "    for i in range(len(word)-cluster_size):\n",
    "        ngram = word[i:i+cluster_size]\n",
    "        wicklefeatures[i,:] = random_idx.id_vector(N, ngram, alphabet, RI_letters,ordered)\n",
    "    wicklefeature = np.sum(wicklefeatures, axis=0)\n",
    "    pickle.dump(wicklefeature, open(filepath+past_tense_word, \"wb\"))\n",
    "    wicklefeatures, wicklefeature\n",
    "   \n",
    "\n",
    "    \n",
    "def get_training_set(set_size, folderpath=\"wickle_train/\", verbtype=\"ed\",filepath=\"ed.txt\"):\n",
    "    past = read_examples(folderpath + filepath)\n",
    "    past = list(random.sample(past, set_size))\n",
    "    X = np.zeros((set_size,N))\n",
    "    for i in range(set_size):\n",
    "        X[i,:] = pickle.load(open(folderpath+verbtype+\"/\"+past[i], \"r\"))\n",
    "    return X\n",
    "    \n",
    "def get_test_set(filepath):\n",
    "    Y = [] # wicklefeatures of the test set\n",
    "    y_labels = [] # 0 for not -ed verb, 1 for -ed verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stage 1. just knowing\n",
    "def typed_training_set():\n",
    "    # all types\n",
    "    simple_present = read_examples(\"wickle_train/simple_present.txt\")\n",
    "    simple_past = read_examples(\"wickle_train/simple_past.txt\")\n",
    "\n",
    "    for word in simple_past:\n",
    "        wicklefeaturize(word, 3, \"wickle_train/typed/\")\n",
    "\n",
    "def training_set(folderpath=\"wickle_train/\", verbtype=\"ed\", filepath=\"ed.txt\"):\n",
    "    # -ed type\n",
    "    # n data points\n",
    "    # y = N x 1 vector predicting if a word is a verb in past tense\n",
    "    past = read_examples(folderpath+filepath)\n",
    "    for p in past:\n",
    "        wicklefeaturize(p, 3, \"wickle_train/\"+verbtype+\"/\")\n",
    "\n",
    "def ed_training_set(folderpath=\"wickle_train/\", verbtype=\"ed\", filepath=\"ed.txt\"):\n",
    "    training_set(folderpath, verbtype, filepath)\n",
    "    \n",
    "def bad_examples_training_set(folderpath=\"wickle_train/\", verbtype=\"non\", filepath=\"nouns5499.txt\"):\n",
    "    # for words that aren't verbs\n",
    "    training_set(folderpath, verbtype, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stage 2. initial prediction\n",
    "# https://github.com/stephencwelch/Neural-Networks-Demystified\n",
    "#New complete class, with changes:\n",
    "class Neural_Network(object):\n",
    "    def __init__(self, Lambda=0):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "        #Regularization Parameter:\n",
    "        self.Lambda = Lambda\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)/X.shape[0] + (self.Lambda/2)*(np.sum(self.W1**2)+np.sum(self.W2**2))\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        #Add gradient of regularization term:\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)/X.shape[0] + self.Lambda*self.W2\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        #Add gradient of regularization term:\n",
    "        dJdW1 = np.dot(X.T, delta2)/X.shape[0] + self.Lambda*self.W1\n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper functions for interacting with other methods/classes\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 Rolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single parameter vector:\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize*self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], \\\n",
    "                             (self.inputLayerSize, self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], \\\n",
    "                             (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n",
    "    \n",
    "##Need to modify trainer class a bit to check testing error during training:\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))\n",
    "        self.testJ.append(self.N.costFunction(self.testX, self.testY))\n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, trainX, trainY, testX, testY):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = trainX\n",
    "        self.y = trainY\n",
    "        \n",
    "        self.testX = testX\n",
    "        self.testY = testY\n",
    "\n",
    "        #Make empty list to store training costs:\n",
    "        self.J = []\n",
    "        self.testJ = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(trainX, trainY), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# ed is type 1, non ed is type 0\n",
    "X = get_training_set(10, \"wickle_train/\", \"ed\", \"ed.txt\")\n",
    "y = np.ones((10,1))\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lambda: regularization parameter\n",
    "NN = Neural_Network(Lambda=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = trainer(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-76c0e4cfeaea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-deaa32e47f7c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'maxiter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'disp'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0m_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostFunctionWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BFGS'\u001b[0m\u001b[0;34m,\u001b[0m                                  \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbackF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quinntran/anaconda/lib/python2.7/site-packages/scipy/optimize/_minimize.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[0;32m/Users/quinntran/anaconda/lib/python2.7/site-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m     \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m     \u001b[0mHk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quinntran/anaconda/lib/python2.7/site-packages/numpy/lib/twodim_base.pyc\u001b[0m in \u001b[0;36meye\u001b[0;34m(N, M, k, dtype)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mM\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "T.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(T.J)\n",
    "grid(1)\n",
    "xlabel('Iterations')\n",
    "ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN.costFunctionPrime(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot cost during training:\n",
    "plot(T.J)\n",
    "plot(T.testJ)\n",
    "grid(1)\n",
    "xlabel('Iterations')\n",
    "ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make sure our gradients our correct after making changes:\n",
    "numgrad = computeNumericalGradient(NN, X, y)\n",
    "grad = NN.computeGradients(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Should be less than 1e-8:\n",
    "norm(grad-numgrad)/norm(grad+numgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arose', 'awoke', 'was', 'bore', 'beat', 'became', 'began', 'bent', 'bet', 'bit', 'bled', 'blew', 'broke', 'brought', 'built', 'burned', 'burst', 'bought', 'caught', 'chose', 'clung', 'came', 'cost', 'crept', 'cut', 'dealt', 'dug', 'dived', 'did', 'drew', 'dreamed', 'drank', 'drove', 'ate', 'fell', 'fed', 'felt', 'fought', 'found', 'fitted', 'fled', 'flung', 'flew', 'forbade', 'forgot', 'forgave', 'forwent', 'froze', 'got', 'gave', 'went', 'ground', 'grew', 'hanged', 'had', 'heard', 'hid', 'hit', 'held', 'hurt', 'kept', 'kneeled', 'knitted', 'knew', 'laid', 'led', 'leaped', 'left', 'lent', 'let', 'lay', 'lighted', 'lost', 'made', 'meant', 'met', 'paid', 'proved', 'put', 'quit', 'read', 'rode', 'rang', 'rose', 'ran', 'sawed', 'said', 'saw', 'sought', 'sold', 'sent', 'set', 'sewed', 'shook', 'shaved', 'sheared', 'shined', 'shot', 'showed', 'shrank', 'shut', 'sang', 'sank', 'sat', 'slew', 'slept', 'slid', 'sneaked', 'spoke', 'sped', 'spent', 'spilled', 'spun', 'spat', 'split', 'spread', 'sprang', 'stood', 'stole', 'stuck', 'stung', 'stank', 'strewed', 'struck', 'strived', 'swore', 'swept', 'swam', 'swung', 'took', 'taught', 'tore', 'told', 'thought', 'thrived', 'threw', 'underwent', 'understood', 'upset', 'waked', 'wore', 'wove', 'wept', 'won', 'wound', 'withdrew', 'wrung', 'wrote']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (10000) into shape (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-05f833154e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msimple_past\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimple_past\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mwicklefeaturize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# -ed type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-05f833154e0e>\u001b[0m in \u001b[0;36mwicklefeaturize\u001b[0;34m(past_tense_word, cluster_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcluster_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mngram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcluster_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mwicklefeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRI_letters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mordered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mwicklefeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwicklefeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwicklefeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wickel_training/typed/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpast_tense_word\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (10000) into shape (3)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# test set is now different\n",
    "# someone clean up test_tense.txt.....\n",
    "test_simple_past = read_examples(\"test_simple_past.txt\")\n",
    "test_tense = read_examples(\"test_tense.txt\")\n",
    "\n",
    "test_simple_present = read_examples(\"test_simple_present.txt\")\n",
    "test_simple_past = read_examples(\"test_simple_past.txt\")\n",
    "\n",
    "\n",
    "# stage 2. predicting tense (regularized, less correct)\n",
    "\n",
    "http://www.myenglishteacher.net/irregular_verbs.html\n",
    "given these few examples, basically 1 example for every rule,\n",
    "generate the past tense for a test word\n",
    "test_tense.txt\n",
    "frequency isn't relevant here because all verbs have frequency ~1\n",
    "\n",
    "n = len(simple_past)\n",
    "d = 10000\n",
    "mu, var = 0, .015\n",
    "noise = np.random.normal(mu, sqrt(var), (n,d))\n",
    "\n",
    "# need the reference for the past tense part of every word argh\n",
    "for i in range(n):\n",
    "    word_hypervec(simple_present[i], alphabet, d)\n",
    "    word_hypervec(simple_past[i], alphabet, d)\n",
    "# model W = weight vectors that represent the part of the word that makes the tense pattern\n",
    "\n",
    "# stage 3. predicting tense, usually correct\n",
    "\n",
    "pg 8 mclellan\n",
    "feature vectors by one hot encoding.\n",
    "how to determine if a verb is irregular or regular. it seems like a specific mapping for irregular. \n",
    "other than -ed and tense endings, what kind of patterns are we using to estimate a verb to a tense?\n",
    "\n",
    "\n",
    "\n",
    "are we running linear regression where the elements of weight vector (every type of tense) determined by gaussians?\n",
    "what would an activation/threshold be?\n",
    "pg 9\n",
    "\n",
    "pg 11: sounds like we're gradient descending\n",
    "\n",
    "One hot encoding means can only store N linearly independent sets of patterns. can introduce noise from a gaussian \n",
    "hyperparameters mu = 0 and variance to have each set represent a general rule so can store more patterns. \n",
    "rule of 78 wat.\n",
    "pg 14\n",
    "\n",
    "enforce logarithmic growth for adding patterns of regular verbs\n",
    "pg 15\n",
    "\n",
    "A scheme which meets the first criterion, but not the second, is the scheme proposed by Wickelgren 0969) .\n",
    "He suggested that words\n",
    "should be represented as sequences of context-sensitive phoneme units\n",
    "which represent each phone in a word as a triple, consisting of the phone itself, its predecessor, and its successor.\n",
    "Notationally, we write each Wickelphone as a triple of phonemes, consisting of the central phoneme,\n",
    "subscripted on the left by its predecessor and on the right by its successor. \n",
    "A phoneme occur- ring at the beginning of a word is preceded by a special symbol (#) standing for the word boundary; \n",
    "pg 18\n",
    "\n",
    "Though the Wickelphones in a word are not strictly position specific , \n",
    "it turns out that (a) few words contain more than one occurrence of any given Wickelphone, \n",
    "and (b) there are no two words we know of that consist of the same sequence of Wickel- phones. \n",
    "For example Islitl and Isiltl contain no Wickelphones in common.\n",
    "One nice property of Wickelphones is that they capture enough of the context in which a phoneme \n",
    "occurs to provide a sufficient basis for differentiating between the different cases of the \n",
    "past-tense rule and for characterizing the contextual variables that determine the subregulari- ties \n",
    "among the irregular past-tense verbs. For example, the word-final phoneme that \n",
    "determines whether we should add Idl, It I or rdl forming the regular past. \n",
    "And it is the sequence iN # which transformed to aN # in the ing ang pattern found in words like sing.\n",
    "The trouble with the Wickelphone solution is that there are too many of them, and they are too specific\n",
    "pg 19\n",
    "\n",
    "for neural net:\n",
    "activation function: sigmoid, wickelphone, relu\n",
    "\n",
    "hat the model captures the basic three-stage pattern of acquisi- tion.\n",
    ". That the model captures most aspects of differences in per- formance on different types of regular and irregular verbs.\n",
    ". That the model is capable of responding appropriately to verbs it has never seen before , as well as to regular and irregular verbs actually experienced during training.\n",
    "The more frequent a verb is, the more likely it is to be a regular verb. \n",
    "pg 25\n",
    "Divide irregular verbs into 9 classes:\n",
    "\n",
    "Verbs that do not change at all to form the past tense.\n",
    "\n",
    "Verbs that change to a final /d/ to /t/ to form the past tense.\n",
    "\n",
    "Verbs that undergo an internal vowel change and also add a final /t/ or /d/.\n",
    "\n",
    "Verbs that undergo an internal vowel change and also a final /t/ or /d/.\n",
    "\n",
    "Verbs that undergo an internal vowel change whose stems end in a dental.\n",
    "\n",
    "Verbs that undergo a an internal vowel change of /i/ or /a/ to /^/.\n",
    "\n",
    "Verbs that undergo a an internal vowel change of /i/ to /a/.\n",
    "\n",
    "All other verbs that undergo an internal vowel change.\n",
    "\n",
    "All verbs that undergo a vowel change and that end in a dipthongal sequence.\n",
    "\n",
    "Divide regular verbs into 3 categories: \n",
    "those ending in a vowel or voiced consonant, which take a /d/ to form the past tense\n",
    "those ending in a voiceless consonant, which take a /d/\n",
    "those ending in /t/ or /d/, which take a final /^d/ to form the past tense\n",
    "\n",
    "how clearly the same patterns evident in the Bybee and Slobin data. Verbs ending in t/d always show a stronger no-change response and a weaker regularized response than those not ending in t/d. During the very early stages of learning, however, the regularized response is stronger than the no-change response-even the verb does end with t/d. This suggests that the generalization that the past tense of tld verbs is formed by adding /^d/ is stronger than the generalization that verbs ending in t/d should not have an ending added. However, as learning proceeds, this secondary generalization is made (though for only a subset of the tl d verbs , as we shall see), and the simulation shows the same interaction that Bybee and Slobin 0982) found iri their preschoolers.\n",
    "\n",
    " pg 35\n",
    "\n",
    "Erroneous no-change responses are clearly stronger for both regular\n",
    "\n",
    "and irregular t/d verbs. that the erroneous no-change responses are stronger for the t/d verbs than for the other types of irregular verbs\n",
    "\n",
    " pg 36\n",
    "\n",
    "\n",
    "\n",
    "Type 1. to have the least errors of irregular verbs\n",
    "\n",
    "Probability of regularization: (base+ed + past+ed) / (base+ed + past+ed + correct)\n",
    "\n",
    "\n",
    "\n",
    "Model should be sensitive to word frequency. guessing that we’re doing some lda shit haha. Their model was always given the present and past tenses together.\n",
    "\n",
    "pg 41\n",
    "\n",
    "\n",
    "\n",
    "pg 42 : Intuition on verb patterns. too much to write lmao\n",
    "\n",
    "pg 43\n",
    "\n",
    "pg 45: response as in what kind of pattern will the past tense version of the verb take. response strength as in frequency of this specific transformation (kind of pattern)\n",
    "\n",
    "pg 46: Examine model’s performance by\n",
    "\n",
    "Overall degree of transfer: how accurately the model generates the correct features of the new verbs.\n",
    "\n",
    " Unconstrained responses: model should not try out a certain set of past tenses for every verb, but should actually estimate the correct tense from a set of all tense patterns (so word patterns not necessarily tenses/ tenses of that word) ???\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
