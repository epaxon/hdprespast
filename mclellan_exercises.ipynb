{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from utils import random_idx\n",
    "from utils import utils\n",
    "from utils import lang_vectors_utils as lvu\n",
    "%matplotlib inline\n",
    "\n",
    "alphabet = string.lowercase + ' _#'\n",
    "k = 5000\n",
    "N = 10000\n",
    "\n",
    "cluster_sizes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "ordered = 1\n",
    "#assuming this is the alphabet bc of precedent in generate_text.py\n",
    "#alph = 'abc' \n",
    "alphabet = string.lowercase + ' '\n",
    "RI_letters = random_idx.generate_letter_id_vectors(N, k, alphabet)\n",
    "# words should be unique? (such as permutations)\n",
    "# number of words added in a syntax vector\n",
    "syntax_granularities = [5,10,100,200]\n",
    "lower_n_cutoff = .85\n",
    "\n",
    "def read_examples(filepath):\n",
    "    examples = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                examples.append(word)\n",
    "    return examples\n",
    "\n",
    "def wicklefeaturize(past_tense_word, cluster_size):\n",
    "    \"\"\"\n",
    "    Create a wicklefeature matrix (for mapping) and vector (for computations).\n",
    "    only trigrams (cluster_size of size 3).\n",
    "    Save the matrix.\n",
    "    \n",
    "    _ j u\n",
    "    j u m\n",
    "    u m p\n",
    "    m p #\n",
    "\n",
    "    \"\"\"\n",
    "    word = \"_\" + past_tense_word + \"#\"\n",
    "    wicklefeatures = np.zeros((len(word)-cluster_size,N))\n",
    "    for i in range(len(word)-cluster_size):\n",
    "        ngram = word[i:i+cluster_size]\n",
    "        wicklefeatures[i,:] = random_idx.id_vector(N, ngram, alphabet, RI_letters,ordered=0)\n",
    "    wicklefeature = np.sum(wicklefeatures, axis=0)\n",
    "    pickle.dump(wicklefeatures, open(\"wickle_training/typed/\"+past_tense_word, \"wb\"))\n",
    "    wicklefeatures, wicklefeature\n",
    "   \n",
    "    \n",
    "# stage 1. just knowing\n",
    "def typed_training_set():\n",
    "    # all types\n",
    "    simple_present = read_examples(\"simple_present.txt\")\n",
    "    simple_past = read_examples(\"simple_past.txt\")\n",
    "\n",
    "    for word in simple_past:\n",
    "        wicklefeaturize(word, 3)\n",
    "\n",
    "def ed_training_set():\n",
    "    # -ed type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arose', 'awoke', 'was', 'bore', 'beat', 'became', 'began', 'bent', 'bet', 'bit', 'bled', 'blew', 'broke', 'brought', 'built', 'burned', 'burst', 'bought', 'caught', 'chose', 'clung', 'came', 'cost', 'crept', 'cut', 'dealt', 'dug', 'dived', 'did', 'drew', 'dreamed', 'drank', 'drove', 'ate', 'fell', 'fed', 'felt', 'fought', 'found', 'fitted', 'fled', 'flung', 'flew', 'forbade', 'forgot', 'forgave', 'forwent', 'froze', 'got', 'gave', 'went', 'ground', 'grew', 'hanged', 'had', 'heard', 'hid', 'hit', 'held', 'hurt', 'kept', 'kneeled', 'knitted', 'knew', 'laid', 'led', 'leaped', 'left', 'lent', 'let', 'lay', 'lighted', 'lost', 'made', 'meant', 'met', 'paid', 'proved', 'put', 'quit', 'read', 'rode', 'rang', 'rose', 'ran', 'sawed', 'said', 'saw', 'sought', 'sold', 'sent', 'set', 'sewed', 'shook', 'shaved', 'sheared', 'shined', 'shot', 'showed', 'shrank', 'shut', 'sang', 'sank', 'sat', 'slew', 'slept', 'slid', 'sneaked', 'spoke', 'sped', 'spent', 'spilled', 'spun', 'spat', 'split', 'spread', 'sprang', 'stood', 'stole', 'stuck', 'stung', 'stank', 'strewed', 'struck', 'strived', 'swore', 'swept', 'swam', 'swung', 'took', 'taught', 'tore', 'told', 'thought', 'thrived', 'threw', 'underwent', 'understood', 'upset', 'waked', 'wore', 'wove', 'wept', 'won', 'wound', 'withdrew', 'wrung', 'wrote']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (10000) into shape (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-05f833154e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msimple_past\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimple_past\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mwicklefeaturize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# -ed type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-05f833154e0e>\u001b[0m in \u001b[0;36mwicklefeaturize\u001b[0;34m(past_tense_word, cluster_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcluster_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mngram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcluster_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mwicklefeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRI_letters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mordered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mwicklefeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwicklefeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwicklefeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wickel_training/typed/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpast_tense_word\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (10000) into shape (3)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# test set is now different\n",
    "# someone clean up test_tense.txt.....\n",
    "test_simple_past = read_examples(\"test_simple_past.txt\")\n",
    "test_tense = read_examples(\"test_tense.txt\")\n",
    "\n",
    "test_simple_present = read_examples(\"test_simple_present.txt\")\n",
    "test_simple_past = read_examples(\"test_simple_past.txt\")\n",
    "\n",
    "\n",
    "# stage 2. predicting tense (regularized, less correct)\n",
    "\n",
    "http://www.myenglishteacher.net/irregular_verbs.html\n",
    "given these few examples, basically 1 example for every rule,\n",
    "generate the past tense for a test word\n",
    "test_tense.txt\n",
    "frequency isn't relevant here because all verbs have frequency ~1\n",
    "\n",
    "n = len(simple_past)\n",
    "d = 10000\n",
    "mu, var = 0, .015\n",
    "noise = np.random.normal(mu, sqrt(var), (n,d))\n",
    "\n",
    "# need the reference for the past tense part of every word argh\n",
    "for i in range(n):\n",
    "    word_hypervec(simple_present[i], alphabet, d)\n",
    "    word_hypervec(simple_past[i], alphabet, d)\n",
    "# model W = weight vectors that represent the part of the word that makes the tense pattern\n",
    "\n",
    "# stage 3. predicting tense, usually correct\n",
    "\n",
    "pg 8 mclellan\n",
    "feature vectors by one hot encoding.\n",
    "how to determine if a verb is irregular or regular. it seems like a specific mapping for irregular. \n",
    "other than -ed and tense endings, what kind of patterns are we using to estimate a verb to a tense?\n",
    "\n",
    "\n",
    "\n",
    "are we running linear regression where the elements of weight vector (every type of tense) determined by gaussians?\n",
    "what would an activation/threshold be?\n",
    "pg 9\n",
    "\n",
    "pg 11: sounds like we're gradient descending\n",
    "\n",
    "One hot encoding means can only store N linearly independent sets of patterns. can introduce noise from a gaussian \n",
    "hyperparameters mu = 0 and variance to have each set represent a general rule so can store more patterns. \n",
    "rule of 78 wat.\n",
    "pg 14\n",
    "\n",
    "enforce logarithmic growth for adding patterns of regular verbs\n",
    "pg 15\n",
    "\n",
    "A scheme which meets the first criterion, but not the second, is the scheme proposed by Wickelgren 0969) .\n",
    "He suggested that words\n",
    "should be represented as sequences of context-sensitive phoneme units\n",
    "which represent each phone in a word as a triple, consisting of the phone itself, its predecessor, and its successor.\n",
    "Notationally, we write each Wickelphone as a triple of phonemes, consisting of the central phoneme,\n",
    "subscripted on the left by its predecessor and on the right by its successor. \n",
    "A phoneme occur- ring at the beginning of a word is preceded by a special symbol (#) standing for the word boundary; \n",
    "pg 18\n",
    "\n",
    "Though the Wickelphones in a word are not strictly position specific , \n",
    "it turns out that (a) few words contain more than one occurrence of any given Wickelphone, \n",
    "and (b) there are no two words we know of that consist of the same sequence of Wickel- phones. \n",
    "For example Islitl and Isiltl contain no Wickelphones in common.\n",
    "One nice property of Wickelphones is that they capture enough of the context in which a phoneme \n",
    "occurs to provide a sufficient basis for differentiating between the different cases of the \n",
    "past-tense rule and for characterizing the contextual variables that determine the subregulari- ties \n",
    "among the irregular past-tense verbs. For example, the word-final phoneme that \n",
    "determines whether we should add Idl, It I or rdl forming the regular past. \n",
    "And it is the sequence iN # which transformed to aN # in the ing ang pattern found in words like sing.\n",
    "The trouble with the Wickelphone solution is that there are too many of them, and they are too specific\n",
    "pg 19\n",
    "\n",
    "for neural net:\n",
    "activation function: sigmoid, wickelphone, relu\n",
    "\n",
    "hat the model captures the basic three-stage pattern of acquisi- tion.\n",
    ". That the model captures most aspects of differences in per- formance on different types of regular and irregular verbs.\n",
    ". That the model is capable of responding appropriately to verbs it has never seen before , as well as to regular and irregular verbs actually experienced during training.\n",
    "The more frequent a verb is, the more likely it is to be a regular verb. \n",
    "pg 25\n",
    "Divide irregular verbs into 9 classes:\n",
    "\n",
    "Verbs that do not change at all to form the past tense.\n",
    "\n",
    "Verbs that change to a final /d/ to /t/ to form the past tense.\n",
    "\n",
    "Verbs that undergo an internal vowel change and also add a final /t/ or /d/.\n",
    "\n",
    "Verbs that undergo an internal vowel change and also a final /t/ or /d/.\n",
    "\n",
    "Verbs that undergo an internal vowel change whose stems end in a dental.\n",
    "\n",
    "Verbs that undergo a an internal vowel change of /i/ or /a/ to /^/.\n",
    "\n",
    "Verbs that undergo a an internal vowel change of /i/ to /a/.\n",
    "\n",
    "All other verbs that undergo an internal vowel change.\n",
    "\n",
    "All verbs that undergo a vowel change and that end in a dipthongal sequence.\n",
    "\n",
    "Divide regular verbs into 3 categories: \n",
    "those ending in a vowel or voiced consonant, which take a /d/ to form the past tense\n",
    "those ending in a voiceless consonant, which take a /d/\n",
    "those ending in /t/ or /d/, which take a final /^d/ to form the past tense\n",
    "\n",
    "how clearly the same patterns evident in the Bybee and Slobin data. Verbs ending in t/d always show a stronger no-change response and a weaker regularized response than those not ending in t/d. During the very early stages of learning, however, the regularized response is stronger than the no-change response-even the verb does end with t/d. This suggests that the generalization that the past tense of tld verbs is formed by adding /^d/ is stronger than the generalization that verbs ending in t/d should not have an ending added. However, as learning proceeds, this secondary generalization is made (though for only a subset of the tl d verbs , as we shall see), and the simulation shows the same interaction that Bybee and Slobin 0982) found iri their preschoolers.\n",
    "\n",
    " pg 35\n",
    "\n",
    "Erroneous no-change responses are clearly stronger for both regular\n",
    "\n",
    "and irregular t/d verbs. that the erroneous no-change responses are stronger for the t/d verbs than for the other types of irregular verbs\n",
    "\n",
    " pg 36\n",
    "\n",
    "\n",
    "\n",
    "Type 1. to have the least errors of irregular verbs\n",
    "\n",
    "Probability of regularization: (base+ed + past+ed) / (base+ed + past+ed + correct)\n",
    "\n",
    "\n",
    "\n",
    "Model should be sensitive to word frequency. guessing that we’re doing some lda shit haha. Their model was always given the present and past tenses together.\n",
    "\n",
    "pg 41\n",
    "\n",
    "\n",
    "\n",
    "pg 42 : Intuition on verb patterns. too much to write lmao\n",
    "\n",
    "pg 43\n",
    "\n",
    "pg 45: response as in what kind of pattern will the past tense version of the verb take. response strength as in frequency of this specific transformation (kind of pattern)\n",
    "\n",
    "pg 46: Examine model’s performance by\n",
    "\n",
    "Overall degree of transfer: how accurately the model generates the correct features of the new verbs.\n",
    "\n",
    " Unconstrained responses: model should not try out a certain set of past tenses for every verb, but should actually estimate the correct tense from a set of all tense patterns (so word patterns not necessarily tenses/ tenses of that word) ???\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
