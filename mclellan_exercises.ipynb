{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import utils\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy import optimize\n",
    "from utils import random_idx\n",
    "from utils import utils\n",
    "from utils import lang_vectors_utils as lvu\n",
    "%matplotlib inline\n",
    "\n",
    "alphabet = string.lowercase + ' _#'\n",
    "k = 5000\n",
    "N = 10000\n",
    "\n",
    "cluster_sizes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "ordered = 1\n",
    "alphabet = string.lowercase + ' '\n",
    "RI_letters = random_idx.generate_letter_id_vectors(N, k, alphabet)\n",
    "lower_n_cutoff = .85\n",
    "\n",
    "def read_examples(filepath):\n",
    "    examples = set()\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                examples.add(word)\n",
    "    return examples\n",
    "\n",
    "def wicklefeaturize(past_tense_word, cluster_size, filepath=\"wickle_train/\"):\n",
    "    \"\"\"\n",
    "    Create a wicklefeature matrix (for mapping) and vector (for computations).\n",
    "    only trigrams (cluster_size of size 3).\n",
    "    Save the matrix.\n",
    "    \n",
    "    _ j u\n",
    "    j u m\n",
    "    u m p\n",
    "    m p #\n",
    "\n",
    "    \"\"\"\n",
    "    word = \"_\" + past_tense_word + \"#\"\n",
    "    wicklefeatures = np.zeros((len(word)-cluster_size,N))\n",
    "    for i in range(len(word)-cluster_size):\n",
    "        ngram = word[i:i+cluster_size]\n",
    "        wicklefeatures[i,:] = random_idx.id_vector(N, ngram, alphabet, RI_letters,ordered)\n",
    "    wicklefeature = np.sum(wicklefeatures, axis=0)\n",
    "    pickle.dump(wicklefeature, open(filepath+past_tense_word, \"wb\"))\n",
    "    wicklefeatures, wicklefeature\n",
    "   \n",
    "\n",
    "    \n",
    "def get_training_set(set_size, folderpath=\"wickle_train/\", verbtype=\"ed\",filepath=\"ed.txt\"):\n",
    "    past = read_examples(folderpath + filepath)\n",
    "    past = list(random.sample(past, set_size))\n",
    "    X = np.zeros((set_size,N))\n",
    "    for i in range(set_size):\n",
    "        X[i,:] = pickle.load(open(folderpath+verbtype+\"/\"+past[i], \"r\"))\n",
    "    return X\n",
    "    \n",
    "def get_test_set(filepath):\n",
    "    Y = [] # wicklefeatures of the test set\n",
    "    y_labels = [] # 0 for not -ed verb, 1 for -ed verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stage 1. just knowing\n",
    "def typed_training_set():\n",
    "    # all types\n",
    "    simple_present = read_examples(\"wickle_train/simple_present.txt\")\n",
    "    simple_past = read_examples(\"wickle_train/simple_past.txt\")\n",
    "\n",
    "    for word in simple_past:\n",
    "        wicklefeaturize(word, 3, \"wickle_train/typed/\")\n",
    "\n",
    "def training_set(folderpath=\"wickle_train/\", verbtype=\"ed\", filepath=\"ed.txt\"):\n",
    "    # -ed type\n",
    "    # n data points\n",
    "    # y = N x 1 vector predicting if a word is a verb in past tense\n",
    "    past = read_examples(folderpath+filepath)\n",
    "    for p in past:\n",
    "        wicklefeaturize(p, 3, \"wickle_train/\"+verbtype+\"/\")\n",
    "\n",
    "def ed_training_set(folderpath=\"wickle_train/\", verbtype=\"ed\", filepath=\"ed.txt\"):\n",
    "    training_set(folderpath, verbtype, filepath)\n",
    "    \n",
    "def bad_examples_training_set(folderpath=\"wickle_train/\", verbtype=\"non\", filepath=\"nouns5499.txt\"):\n",
    "    # for words that aren't verbs\n",
    "    training_set(folderpath, verbtype, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stage 2. initial prediction\n",
    "# https://github.com/stephencwelch/Neural-Networks-Demystified\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        # Define Hyperparameters\n",
    "        self.inputLayerSize = N\n",
    "        self.outputLayerSize = N\n",
    "        self.hiddenLayerSize = N\n",
    "        \n",
    "        # Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propagate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3)\n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        # Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        #least squares\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n",
    "        \n",
    "    def computeNumericalGradient(N, X, y):\n",
    "        paramsInitial = N.getParams()\n",
    "        numgrad = np.zeros(paramsInitial.shape)\n",
    "        perturb = np.zeros(paramsInitial.shape)\n",
    "        e = 1e-4\n",
    "\n",
    "        for p in range(len(paramsInitial)):\n",
    "            #Set perturbation vector\n",
    "            perturb[p] = e\n",
    "            N.setParams(paramsInitial + perturb)\n",
    "            loss2 = N.costFunction(X, y)\n",
    "            \n",
    "            N.setParams(paramsInitial - perturb)\n",
    "            loss1 = N.costFunction(X, y)\n",
    "\n",
    "            #Compute Numerical Gradient\n",
    "            numgrad[p] = (loss2 - loss1) / (2*e)\n",
    "\n",
    "            #Return the value we changed to zero:\n",
    "            perturb[p] = 0\n",
    "            \n",
    "        #Return Params to original value:\n",
    "        N.setParams(paramsInitial)\n",
    "\n",
    "        return numgrad \n",
    "    \n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# ed is type 1, non ed is type 0\n",
    "X = get_training_set(10, \"wickle_train/\", \"ed\", \"ed.txt\")\n",
    "y = np.ones((10,1))\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = trainer(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-76c0e4cfeaea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-deaa32e47f7c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'maxiter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'disp'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0m_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostFunctionWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BFGS'\u001b[0m\u001b[0;34m,\u001b[0m                                  \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbackF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quinntran/anaconda/lib/python2.7/site-packages/scipy/optimize/_minimize.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[0;32m/Users/quinntran/anaconda/lib/python2.7/site-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m     \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m     \u001b[0mHk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quinntran/anaconda/lib/python2.7/site-packages/numpy/lib/twodim_base.pyc\u001b[0m in \u001b[0;36meye\u001b[0;34m(N, M, k, dtype)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mM\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "T.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(T.J)\n",
    "grid(1)\n",
    "xlabel('Iterations')\n",
    "ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STOP FROM HERE ON\n",
    "def ed_init_predict(folderpath=\"wickle_train/\"):\n",
    "    # each data point is an Xi in dataset X, Xi = (len(word)-cluster_size,N) matrix\n",
    "    # but we're actually doing operations on sum of rows of Xi. call this xi.\n",
    "    # output weight matrix W = N x N\n",
    "    # IN PROGRESS\n",
    "    X = get_training_set(1000, folderpath, \"ed\", \"ed.txt\")\n",
    "    nn = Neural_Network()\n",
    "    # each element is 0/1 for past tense verb/na (or is it the other way around?)\n",
    "    # should be all -ed as in all 1's???\n",
    "    #yHat = nn.forward(X)\n",
    "    #pickle.dump(yHat, open(\"wickle_predict/ed/prediction\", \"wb\"))\n",
    "    #return yHat\n",
    "\n",
    "#ed_init_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN.costFunctionPrime(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DATA THINGS HERE ERMAGERD\n",
    "#Test network for various combinations of sleep/study:\n",
    "hoursSleep = linspace(0, 10, 100)\n",
    "hoursStudy = linspace(0, 5, 100)\n",
    "\n",
    "#Normalize data (same way training data way normalized)\n",
    "hoursSleepNorm = hoursSleep/10.\n",
    "hoursStudyNorm = hoursStudy/5.\n",
    "\n",
    "#Create 2-d versions of input for plotting\n",
    "a, b  = meshgrid(hoursSleepNorm, hoursStudyNorm)\n",
    "\n",
    "#Join into a single input matrix:\n",
    "allInputs = np.zeros((a.size, 2))\n",
    "allInputs[:, 0] = a.ravel()\n",
    "allInputs[:, 1] = b.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allOutputs = NN.forward(allInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Contour Plot:\n",
    "yy = np.dot(hoursStudy.reshape(100,1), np.ones((1,100)))\n",
    "xx = np.dot(hoursSleep.reshape(100,1), np.ones((1,100))).T\n",
    "\n",
    "CS = contour(xx,yy,100*allOutputs.reshape(100, 100))\n",
    "clabel(CS, inline=1, fontsize=10)\n",
    "xlabel('Hours Sleep')\n",
    "ylabel('Hours Study')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3D plot:\n",
    "\n",
    "##Uncomment to plot out-of-notebook (you'll be able to rotate)\n",
    "#%matplotlib qt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(xx, yy, 100*allOutputs.reshape(100, 100), \\\n",
    "                       cmap=cm.jet)\n",
    "\n",
    "ax.set_xlabel('Hours Sleep')\n",
    "ax.set_ylabel('Hours Study')\n",
    "ax.set_zlabel('Test Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = (hours sleeping, hours studying), y = Score on test\n",
    "X = np.array(([3,5], [5,1], [10,2], [6,1.5]), dtype=float)\n",
    "y = np.array(([75], [82], [93], [70]), dtype=float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot projections of our new data:\n",
    "fig = figure(0,(8,3))\n",
    "\n",
    "subplot(1,2,1)\n",
    "scatter(X[:,0], y)\n",
    "grid(1)\n",
    "xlabel('Hours Sleeping')\n",
    "ylabel('Test Score')\n",
    "\n",
    "subplot(1,2,2)\n",
    "scatter(X[:,1], y)\n",
    "grid(1)\n",
    "xlabel('Hours Studying')\n",
    "ylabel('Test Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize\n",
    "X = X/np.amax(X, axis=0)\n",
    "y = y/100 #Max test score is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train network with new data:\n",
    "T = trainer(NN)\n",
    "T.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot cost during training:\n",
    "plot(T.J)\n",
    "grid(1)\n",
    "xlabel('Iterations')\n",
    "ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test network for various combinations of sleep/study:\n",
    "hoursSleep = linspace(0, 10, 100)\n",
    "hoursStudy = linspace(0, 5, 100)\n",
    "\n",
    "#Normalize data (same way training data way normalized)\n",
    "hoursSleepNorm = hoursSleep/10.\n",
    "hoursStudyNorm = hoursStudy/5.\n",
    "\n",
    "#Create 2-d versions of input for plotting\n",
    "a, b  = meshgrid(hoursSleepNorm, hoursStudyNorm)\n",
    "\n",
    "#Join into a single input matrix:\n",
    "allInputs = np.zeros((a.size, 2))\n",
    "allInputs[:, 0] = a.ravel()\n",
    "allInputs[:, 1] = b.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allOutputs = NN.forward(allInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Contour Plot:\n",
    "yy = np.dot(hoursStudy.reshape(100,1), np.ones((1,100)))\n",
    "xx = np.dot(hoursSleep.reshape(100,1), np.ones((1,100))).T\n",
    "\n",
    "CS = contour(xx,yy,100*allOutputs.reshape(100, 100))\n",
    "clabel(CS, inline=1, fontsize=10)\n",
    "xlabel('Hours Sleep')\n",
    "ylabel('Hours Study')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3D plot:\n",
    "#Uncomment to plot out-of-notebook (you'll be able to rotate)\n",
    "#%matplotlib qt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "#Scatter training examples:\n",
    "ax.scatter(10*X[:,0], 5*X[:,1], 100*y, c='k', alpha = 1, s=30)\n",
    "\n",
    "\n",
    "surf = ax.plot_surface(xx, yy, 100*allOutputs.reshape(100, 100), \\\n",
    "                       cmap=cm.jet, alpha = 0.5)\n",
    "\n",
    "\n",
    "ax.set_xlabel('Hours Sleep')\n",
    "ax.set_ylabel('Hours Study')\n",
    "ax.set_zlabel('Test Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training Data:\n",
    "trainX = np.array(([3,5], [5,1], [10,2], [6,1.5]), dtype=float)\n",
    "trainY = np.array(([75], [82], [93], [70]), dtype=float)\n",
    "\n",
    "#Testing Data:\n",
    "testX = np.array(([4, 5.5], [4.5,1], [9,2.5], [6, 2]), dtype=float)\n",
    "testY = np.array(([70], [89], [85], [75]), dtype=float)\n",
    "\n",
    "#Normalize:\n",
    "trainX = trainX/np.amax(trainX, axis=0)\n",
    "trainY = trainY/100 #Max test score is 100\n",
    "\n",
    "#Normalize by max of training data:\n",
    "testX = testX/np.amax(trainX, axis=0)\n",
    "testY = testY/100 #Max test score is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Need to modify trainer class a bit to check testing error during training:\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))\n",
    "        self.testJ.append(self.N.costFunction(self.testX, self.testY))\n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, trainX, trainY, testX, testY):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = trainX\n",
    "        self.y = trainY\n",
    "        \n",
    "        self.testX = testX\n",
    "        self.testY = testY\n",
    "\n",
    "        #Make empty list to store training costs:\n",
    "        self.J = []\n",
    "        self.testJ = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(trainX, trainY), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train network with new data:\n",
    "NN = Neural_Network()\n",
    "\n",
    "T = trainer(NN)\n",
    "T.train(trainX, trainY, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot cost during training:\n",
    "plot(T.J)\n",
    "plot(T.testJ)\n",
    "grid(1)\n",
    "xlabel('Iterations')\n",
    "ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Regularization Parameter:\n",
    "Lambda = 0.0001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Need to make changes to costFunction and costFunctionPrim:\n",
    "def costFunction(self, X, y):\n",
    "    #Compute cost for given X,y, use weights already stored in class.\n",
    "    self.yHat = self.forward(X)\n",
    "    #We don't want cost to increase with the number of examples, so normalize by dividing the error term by number of examples(X.shape[0])\n",
    "    J = 0.5*sum((y-self.yHat)**2)/X.shape[0] + (self.Lambda/2)*(sum(self.W1**2)+sum(self.W2**2))\n",
    "    return J\n",
    "\n",
    "def costFunctionPrime(self, X, y):\n",
    "    #Compute derivative with respect to W and W2 for a given X and y:\n",
    "    self.yHat = self.forward(X)\n",
    "\n",
    "    delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "    #Add gradient of regularization term:\n",
    "    dJdW2 = np.dot(self.a2.T, delta3)/X.shape[0] + self.Lambda*self.W2\n",
    "\n",
    "    delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "    #Add gradient of regularization term:\n",
    "    dJdW1 = np.dot(X.T, delta2)/X.shape[0] + self.Lambda*self.W1\n",
    "\n",
    "    return dJdW1, dJdW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#New complete class, with changes:\n",
    "class Neural_Network(object):\n",
    "    def __init__(self, Lambda=0):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "        #Regularization Parameter:\n",
    "        self.Lambda = Lambda\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)/X.shape[0] + (self.Lambda/2)*(np.sum(self.W1**2)+np.sum(self.W2**2))\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        #Add gradient of regularization term:\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)/X.shape[0] + self.Lambda*self.W2\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        #Add gradient of regularization term:\n",
    "        dJdW1 = np.dot(X.T, delta2)/X.shape[0] + self.Lambda*self.W1\n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper functions for interacting with other methods/classes\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 Rolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single parameter vector:\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize*self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], \\\n",
    "                             (self.inputLayerSize, self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], \\\n",
    "                             (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network(Lambda=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make sure our gradients our correct after making changes:\n",
    "numgrad = computeNumericalGradient(NN, X, y)\n",
    "grad = NN.computeGradients(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Should be less than 1e-8:\n",
    "norm(grad-numgrad)/norm(grad+numgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = trainer(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T.train(X,y,testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(T.J)\n",
    "plot(T.testJ)\n",
    "grid(1)\n",
    "xlabel('Iterations')\n",
    "ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allOutputs = NN.forward(allInputs)\n",
    "\n",
    "#Contour Plot:\n",
    "yy = np.dot(hoursStudy.reshape(100,1), np.ones((1,100)))\n",
    "xx = np.dot(hoursSleep.reshape(100,1), np.ones((1,100))).T\n",
    "\n",
    "CS = contour(xx,yy,100*allOutputs.reshape(100, 100))\n",
    "clabel(CS, inline=1, fontsize=10)\n",
    "xlabel('Hours Sleep')\n",
    "ylabel('Hours Study')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3D plot:\n",
    "\n",
    "##Uncomment to plot out-of-notebook (you'll be able to rotate)\n",
    "#%matplotlib qt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.scatter(10*X[:,0], 5*X[:,1], 100*y, c='k', alpha = 1, s=30)\n",
    "\n",
    "\n",
    "surf = ax.plot_surface(xx, yy, 100*allOutputs.reshape(100, 100), \\\n",
    "                       cmap=cm.jet, alpha = 0.5)\n",
    "\n",
    "ax.set_xlabel('Hours Sleep')\n",
    "ax.set_ylabel('Hours Study')\n",
    "ax.set_zlabel('Test Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arose', 'awoke', 'was', 'bore', 'beat', 'became', 'began', 'bent', 'bet', 'bit', 'bled', 'blew', 'broke', 'brought', 'built', 'burned', 'burst', 'bought', 'caught', 'chose', 'clung', 'came', 'cost', 'crept', 'cut', 'dealt', 'dug', 'dived', 'did', 'drew', 'dreamed', 'drank', 'drove', 'ate', 'fell', 'fed', 'felt', 'fought', 'found', 'fitted', 'fled', 'flung', 'flew', 'forbade', 'forgot', 'forgave', 'forwent', 'froze', 'got', 'gave', 'went', 'ground', 'grew', 'hanged', 'had', 'heard', 'hid', 'hit', 'held', 'hurt', 'kept', 'kneeled', 'knitted', 'knew', 'laid', 'led', 'leaped', 'left', 'lent', 'let', 'lay', 'lighted', 'lost', 'made', 'meant', 'met', 'paid', 'proved', 'put', 'quit', 'read', 'rode', 'rang', 'rose', 'ran', 'sawed', 'said', 'saw', 'sought', 'sold', 'sent', 'set', 'sewed', 'shook', 'shaved', 'sheared', 'shined', 'shot', 'showed', 'shrank', 'shut', 'sang', 'sank', 'sat', 'slew', 'slept', 'slid', 'sneaked', 'spoke', 'sped', 'spent', 'spilled', 'spun', 'spat', 'split', 'spread', 'sprang', 'stood', 'stole', 'stuck', 'stung', 'stank', 'strewed', 'struck', 'strived', 'swore', 'swept', 'swam', 'swung', 'took', 'taught', 'tore', 'told', 'thought', 'thrived', 'threw', 'underwent', 'understood', 'upset', 'waked', 'wore', 'wove', 'wept', 'won', 'wound', 'withdrew', 'wrung', 'wrote']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (10000) into shape (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-05f833154e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msimple_past\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimple_past\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mwicklefeaturize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# -ed type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-05f833154e0e>\u001b[0m in \u001b[0;36mwicklefeaturize\u001b[0;34m(past_tense_word, cluster_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcluster_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mngram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcluster_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mwicklefeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRI_letters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mordered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mwicklefeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwicklefeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwicklefeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wickel_training/typed/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpast_tense_word\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (10000) into shape (3)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# test set is now different\n",
    "# someone clean up test_tense.txt.....\n",
    "test_simple_past = read_examples(\"test_simple_past.txt\")\n",
    "test_tense = read_examples(\"test_tense.txt\")\n",
    "\n",
    "test_simple_present = read_examples(\"test_simple_present.txt\")\n",
    "test_simple_past = read_examples(\"test_simple_past.txt\")\n",
    "\n",
    "\n",
    "# stage 2. predicting tense (regularized, less correct)\n",
    "\n",
    "http://www.myenglishteacher.net/irregular_verbs.html\n",
    "given these few examples, basically 1 example for every rule,\n",
    "generate the past tense for a test word\n",
    "test_tense.txt\n",
    "frequency isn't relevant here because all verbs have frequency ~1\n",
    "\n",
    "n = len(simple_past)\n",
    "d = 10000\n",
    "mu, var = 0, .015\n",
    "noise = np.random.normal(mu, sqrt(var), (n,d))\n",
    "\n",
    "# need the reference for the past tense part of every word argh\n",
    "for i in range(n):\n",
    "    word_hypervec(simple_present[i], alphabet, d)\n",
    "    word_hypervec(simple_past[i], alphabet, d)\n",
    "# model W = weight vectors that represent the part of the word that makes the tense pattern\n",
    "\n",
    "# stage 3. predicting tense, usually correct\n",
    "\n",
    "pg 8 mclellan\n",
    "feature vectors by one hot encoding.\n",
    "how to determine if a verb is irregular or regular. it seems like a specific mapping for irregular. \n",
    "other than -ed and tense endings, what kind of patterns are we using to estimate a verb to a tense?\n",
    "\n",
    "\n",
    "\n",
    "are we running linear regression where the elements of weight vector (every type of tense) determined by gaussians?\n",
    "what would an activation/threshold be?\n",
    "pg 9\n",
    "\n",
    "pg 11: sounds like we're gradient descending\n",
    "\n",
    "One hot encoding means can only store N linearly independent sets of patterns. can introduce noise from a gaussian \n",
    "hyperparameters mu = 0 and variance to have each set represent a general rule so can store more patterns. \n",
    "rule of 78 wat.\n",
    "pg 14\n",
    "\n",
    "enforce logarithmic growth for adding patterns of regular verbs\n",
    "pg 15\n",
    "\n",
    "A scheme which meets the first criterion, but not the second, is the scheme proposed by Wickelgren 0969) .\n",
    "He suggested that words\n",
    "should be represented as sequences of context-sensitive phoneme units\n",
    "which represent each phone in a word as a triple, consisting of the phone itself, its predecessor, and its successor.\n",
    "Notationally, we write each Wickelphone as a triple of phonemes, consisting of the central phoneme,\n",
    "subscripted on the left by its predecessor and on the right by its successor. \n",
    "A phoneme occur- ring at the beginning of a word is preceded by a special symbol (#) standing for the word boundary; \n",
    "pg 18\n",
    "\n",
    "Though the Wickelphones in a word are not strictly position specific , \n",
    "it turns out that (a) few words contain more than one occurrence of any given Wickelphone, \n",
    "and (b) there are no two words we know of that consist of the same sequence of Wickel- phones. \n",
    "For example Islitl and Isiltl contain no Wickelphones in common.\n",
    "One nice property of Wickelphones is that they capture enough of the context in which a phoneme \n",
    "occurs to provide a sufficient basis for differentiating between the different cases of the \n",
    "past-tense rule and for characterizing the contextual variables that determine the subregulari- ties \n",
    "among the irregular past-tense verbs. For example, the word-final phoneme that \n",
    "determines whether we should add Idl, It I or rdl forming the regular past. \n",
    "And it is the sequence iN # which transformed to aN # in the ing ang pattern found in words like sing.\n",
    "The trouble with the Wickelphone solution is that there are too many of them, and they are too specific\n",
    "pg 19\n",
    "\n",
    "for neural net:\n",
    "activation function: sigmoid, wickelphone, relu\n",
    "\n",
    "hat the model captures the basic three-stage pattern of acquisi- tion.\n",
    ". That the model captures most aspects of differences in per- formance on different types of regular and irregular verbs.\n",
    ". That the model is capable of responding appropriately to verbs it has never seen before , as well as to regular and irregular verbs actually experienced during training.\n",
    "The more frequent a verb is, the more likely it is to be a regular verb. \n",
    "pg 25\n",
    "Divide irregular verbs into 9 classes:\n",
    "\n",
    "Verbs that do not change at all to form the past tense.\n",
    "\n",
    "Verbs that change to a final /d/ to /t/ to form the past tense.\n",
    "\n",
    "Verbs that undergo an internal vowel change and also add a final /t/ or /d/.\n",
    "\n",
    "Verbs that undergo an internal vowel change and also a final /t/ or /d/.\n",
    "\n",
    "Verbs that undergo an internal vowel change whose stems end in a dental.\n",
    "\n",
    "Verbs that undergo a an internal vowel change of /i/ or /a/ to /^/.\n",
    "\n",
    "Verbs that undergo a an internal vowel change of /i/ to /a/.\n",
    "\n",
    "All other verbs that undergo an internal vowel change.\n",
    "\n",
    "All verbs that undergo a vowel change and that end in a dipthongal sequence.\n",
    "\n",
    "Divide regular verbs into 3 categories: \n",
    "those ending in a vowel or voiced consonant, which take a /d/ to form the past tense\n",
    "those ending in a voiceless consonant, which take a /d/\n",
    "those ending in /t/ or /d/, which take a final /^d/ to form the past tense\n",
    "\n",
    "how clearly the same patterns evident in the Bybee and Slobin data. Verbs ending in t/d always show a stronger no-change response and a weaker regularized response than those not ending in t/d. During the very early stages of learning, however, the regularized response is stronger than the no-change response-even the verb does end with t/d. This suggests that the generalization that the past tense of tld verbs is formed by adding /^d/ is stronger than the generalization that verbs ending in t/d should not have an ending added. However, as learning proceeds, this secondary generalization is made (though for only a subset of the tl d verbs , as we shall see), and the simulation shows the same interaction that Bybee and Slobin 0982) found iri their preschoolers.\n",
    "\n",
    " pg 35\n",
    "\n",
    "Erroneous no-change responses are clearly stronger for both regular\n",
    "\n",
    "and irregular t/d verbs. that the erroneous no-change responses are stronger for the t/d verbs than for the other types of irregular verbs\n",
    "\n",
    " pg 36\n",
    "\n",
    "\n",
    "\n",
    "Type 1. to have the least errors of irregular verbs\n",
    "\n",
    "Probability of regularization: (base+ed + past+ed) / (base+ed + past+ed + correct)\n",
    "\n",
    "\n",
    "\n",
    "Model should be sensitive to word frequency. guessing that we’re doing some lda shit haha. Their model was always given the present and past tenses together.\n",
    "\n",
    "pg 41\n",
    "\n",
    "\n",
    "\n",
    "pg 42 : Intuition on verb patterns. too much to write lmao\n",
    "\n",
    "pg 43\n",
    "\n",
    "pg 45: response as in what kind of pattern will the past tense version of the verb take. response strength as in frequency of this specific transformation (kind of pattern)\n",
    "\n",
    "pg 46: Examine model’s performance by\n",
    "\n",
    "Overall degree of transfer: how accurately the model generates the correct features of the new verbs.\n",
    "\n",
    " Unconstrained responses: model should not try out a certain set of past tenses for every verb, but should actually estimate the correct tense from a set of all tense patterns (so word patterns not necessarily tenses/ tenses of that word) ???\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
