{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from pylab import *\n",
    "import scipy\n",
    "import time\n",
    "import utils\n",
    "import hrr_utils\n",
    "\n",
    "#import imnet_utils as imut\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA, FastICA, TruncatedSVD, NMF\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update({'axes.titlesize': 'xx-large'})\n",
    "plt.rcParams.update({'axes.labelsize': 'xx-large'})\n",
    "plt.rcParams.update({'xtick.labelsize': 'x-large', 'ytick.labelsize': 'x-large'})\n",
    "plt.rcParams.update({'legend.fontsize': 'x-large'})\n",
    "plt.rcParams.update({'text.usetex': True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(past, present, dic1, dic2, ngram_length=3):\n",
    "    past1 = hrr_utils.GetRVClipHash(past, dic1, ngram_length)\n",
    "    past2 = hrr_utils.GetRVClipHash(past, dic2, ngram_length)\n",
    "    pres1 = hrr_utils.GetRVClipHash(present, dic1, ngram_length)\n",
    "    pres2 = hrr_utils.GetRVClipHash(present, dic2, ngram_length)\n",
    "    #X = np.where(PAST1-PRES1 > 0, 1, -1)\n",
    "    return past1, pres1, past2, pres2\n",
    "\n",
    "def ngram_encode(ngram_str, letter_vecs, alph, window=3):\n",
    "    vec = np.zeros(letter_vecs.shape[1])\n",
    "    \n",
    "    full_str = '#' + ngram_str + '.'\n",
    "    \n",
    "    \n",
    "    for il, l in enumerate(full_str[:-(window-1)]):\n",
    "        trivec = letter_vecs[alph.find(full_str[il]), :]\n",
    "        for c3 in range(1, window):\n",
    "            trivec = trivec * np.roll(letter_vecs[alph.find(full_str[il+c3]), :], c3)\n",
    "            \n",
    "        vec += trivec\n",
    "    return vec\n",
    "\n",
    "def ngram_encode_cl(ngram_str, letter_vecs, alph, window=3):\n",
    "    vec = np.zeros(letter_vecs.shape[1])\n",
    "    \n",
    "    full_str = '#' + ngram_str + '.'\n",
    "    \n",
    "    for il, l in enumerate(full_str[:-(window-1)]):\n",
    "        trivec = letter_vecs[alph.find(full_str[il]), :]\n",
    "        for c3 in range(1, window):\n",
    "            trivec = trivec * np.roll(letter_vecs[alph.find(full_str[il+c3]), :], c3)\n",
    "            \n",
    "        vec += trivec\n",
    "    return 2* (vec + 0.1*(np.random.rand(letter_vecs.shape[1])-0.5) < 0) - 1\n",
    "   \n",
    "def bind(past, present):\n",
    "    return np.multiply(past, present)\n",
    "\n",
    "def reg_bind(past, present):\n",
    "    pred = np.multiply(tv, present)\n",
    "    return ((N-sim(pred, past))/float(N)) * np.multiply(past, present)\n",
    "\n",
    "def bind_diff(past2, present1, present2):\n",
    "    return np.multiply(present1, past2-present2)\n",
    "\n",
    "def reg_bind_diff(past2, present1, present2):\n",
    "    pred = np.multiply(tv, present1) + present2\n",
    "    return ((N-sim(pred, past2))/float(N)) * np.multiply(past2-present2, present1)\n",
    "\n",
    "def closed_bind(past, present):\n",
    "    return np.dot(np.linalg.pinv(present), past)\n",
    "\n",
    "def sim(x, y):\n",
    "    if len(x.shape) == 1 or len(y.shape)==1:\n",
    "        return np.dot(x, y)\n",
    "    return np.sum(np.multiply(x, y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alph = 'abcdefghijklmnopqrstuvwxyz#.'\n",
    "\n",
    "N = 1000\n",
    "D = len(alph)\n",
    "n_steps = 100\n",
    "letter_vecs = 2 * (np.random.randn(D, N) < 0) - 1\n",
    "\n",
    "dic1 = hrr_utils.GenerateDefaultDictionary(N)\n",
    "dic2 = hrr_utils.GenerateDefaultDictionary(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_encode_train(ngram_strs, encode):\n",
    "    psi = np.zeros(N)\n",
    "    for i in range(len(ngram_strs)):\n",
    "        psi += encode(ngram_strs[i], letter_vecs, alph)\n",
    "    return psi\n",
    "\n",
    "def train(past, present, bind):\n",
    "    psi = np.zeros(N)\n",
    "    for i in range(len(past)):\n",
    "        psi += bind(past[i], present[i])\n",
    "    return psi\n",
    "\n",
    "def diff_train(past2, present1, present2, bind):\n",
    "    psi = np.zeros(N)\n",
    "    for i in range(len(past2)):\n",
    "        psi += bind(past2[i], present1[i], present2[i])\n",
    "    return psi\n",
    "\n",
    "def predict_coefs(n_steps, ngram_str, bound_vec):\n",
    "    l_states = np.random.randn(len(ngram_str)+2, N)\n",
    "    for i in range(1,l_states.shape[0]-1):\n",
    "        l_states[i] = np.dot(letter_vecs.T, np.dot(l_states[i], letter_vecs.T))\n",
    "        l_states[i] = l_states[i]/norm(l_states[i])\n",
    "\n",
    "    l_states[0] = letter_vecs[alph.find('#'), :]\n",
    "    l_states[l_states.shape[0]-1] = letter_vecs[alph.find('.'), :]\n",
    "\n",
    "    l_coef_hists = np.zeros((n_steps, l_states.shape[0], D))\n",
    "    for i in range(n_steps):\n",
    "        for j in range(1,l_states.shape[0]-1):\n",
    "            l_coef_hists[i, j, :] = np.dot(letter_vecs, l_states[j])\n",
    "\n",
    "            ## Need to make sure that the largest absolute value is always positive,\n",
    "            ## because the unbinding inference can flip 2 and be the same\n",
    "\n",
    "            mxjidx = np.argmax(np.abs(l_coef_hists[i, j, :]))\n",
    "            l_states[j] *= np.sign(l_coef_hists[i, j, mxjidx])\n",
    "            if j == 1:\n",
    "                ljd = (np.roll(bound_vec * l_states[j-1] * np.roll(l_states[j+1], 2), -1) +\n",
    "                  bound_vec * np.roll(l_states[j+1], 1) * np.roll(l_states[j+2], 2))\n",
    "            elif 1 < j < l_states.shape[0]-2:\n",
    "                ljd = (np.roll(bound_vec * l_states[j-2] * np.roll(l_states[j-1], 1), -2) +\n",
    "                    np.roll(bound_vec * l_states[j-1] * np.roll(l_states[j+1], 2), -1) +\n",
    "                        bound_vec * np.roll(l_states[j+1], 1) * np.roll(l_states[j+2], 2))\n",
    "            else:\n",
    "                ljd = (np.roll(bound_vec * l_states[j-1] * np.roll(l_states[j+1], 2), -1) +\n",
    "                       np.roll(bound_vec * l_states[j-2] * np.roll(l_states[j-1], 1), -2))\n",
    "\n",
    "            l_states[j] = np.dot(letter_vecs.T, np.dot(ljd, letter_vecs.T))\n",
    "            l_states[j] = l_states[j]/norm(l_states[j])\n",
    "    return l_states, l_coef_hists\n",
    "\n",
    "def graph(n_steps, l_coef_hists):\n",
    "    figure(figsize=(10,3))\n",
    "    cols = get_cmap('copper', n_steps)\n",
    "    for i in range(n_steps):\n",
    "        for j in range(1,l_coef_hists.shape[1]-1):\n",
    "            subplot(130+j)\n",
    "            plot(abs(l_coef_hists[i,j,:]), lw=3, c=cols(i))\n",
    "    \n",
    "def predict(l_coef_hists):\n",
    "    prediction = \"\"\n",
    "    for i in range(1,l_coef_hists.shape[1]-1):\n",
    "        prediction += alph[np.argmax(abs(l_coef_hists[-1,i,:]))]\n",
    "    print prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing noise tolerance\n",
    "# does this mean that bound vec specific to a word \n",
    "# has the same dot product for every correct pair????\n",
    "#ngram_str = 'banana'#'the'\n",
    "ngram_str = 'the'\n",
    "train_set = ['the', 'fam', 'sailboat', 'nuclear', 'compulsive', 'whatisthis']\n",
    "ngram_vecs = [ngram_encode_cl(verb, letter_vecs, alph) for verb in train_set]\n",
    "psi = ngram_encode_train(train_set, ngram_encode)\n",
    "similarities = np.zeros(len(train_set)**2)\n",
    "\n",
    "#figure()\n",
    "for i in range(len(ngram_vecs)):\n",
    "    bound_vec = psi*ngram_vecs[i]\n",
    "    bound_vec = 2* (bound_vec + 0.1*(np.random.rand(letter_vecs.shape[1])-0.5) < 0) - 1\n",
    "    #print bound_vec\n",
    "    for j in range(len(ngram_vecs)):\n",
    "        #if i == j:\n",
    "        similarities[i+j] = bound_vec.dot(ngram_vecs[j])\n",
    "        #print (train_set[i], train_set[j], similarities[i+j])\n",
    "#plot(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1000,), (1000,), (1000,), (1000,))\n",
      "((1000,), (1000,), (1000,), (1000,))\n",
      "iqsqvc\n"
     ]
    }
   ],
   "source": [
    "#ngram_str = 'banana'#'the'\n",
    "past = ['ran', 'walked']\n",
    "present = ['run', 'walk']\n",
    "past1, pres1, past2, pres2 = [], [], [], []\n",
    "for i in range(len(past)):\n",
    "    pa1, pr1, pa2, pr2 = encode(past[i], present[i], dic1, dic2, ngram_length=3)\n",
    "    print (pa1.shape, pr2.shape, pa2.shape, pr2.shape)\n",
    "    past1.append(pa1)\n",
    "    pres1.append(pr1)\n",
    "    past2.append(pa2)\n",
    "    pres2.append(pr2)\n",
    "    \n",
    "#psi = ngram_encode_train(['banana'], ngram_encode)\n",
    "psis = [\n",
    "    train(past1, pres1, bind),\n",
    "    diff_train(past2, pres1, pres2, bind_diff),\n",
    "]\n",
    "\n",
    "ngram_str = past[1]\n",
    "bound_vec = psi[1]\n",
    "#bound_vec = psis[0]*pres1[1]\n",
    "#bound_vec = 2* (bound_vec + 0.1*(np.random.rand(letter_vecs.shape[1])-0.5) < 0) - 1\n",
    "\n",
    "l_states, l_coef_hists = predict_coefs(n_steps, ngram_str, bound_vec)\n",
    "#graph(n_steps, l_coef_hists)\n",
    "predict(l_coef_hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
