{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from pylab import *\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "#import imnet_utils as imut\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA, FastICA, TruncatedSVD, NMF\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update({'axes.titlesize': 'xx-large'})\n",
    "plt.rcParams.update({'axes.labelsize': 'xx-large'})\n",
    "plt.rcParams.update({'xtick.labelsize': 'x-large', 'ytick.labelsize': 'x-large'})\n",
    "plt.rcParams.update({'legend.fontsize': 'x-large'})\n",
    "plt.rcParams.update({'text.usetex': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_encode(ngram_str, letter_vecs, alph, window=3):\n",
    "    vec = np.zeros(letter_vecs.shape[1])\n",
    "    \n",
    "    full_str = '#' + ngram_str + '.'\n",
    "    \n",
    "    \n",
    "    for il, l in enumerate(full_str[:-(window-1)]):\n",
    "        trivec = letter_vecs[alph.find(full_str[il]), :]\n",
    "        for c3 in range(1, window):\n",
    "            trivec = trivec * np.roll(letter_vecs[alph.find(full_str[il+c3]), :], c3)\n",
    "            \n",
    "        vec += trivec\n",
    "    return vec\n",
    "\n",
    "def ngram_encode_cl(ngram_str, letter_vecs, alph, window=3):\n",
    "    vec = np.zeros(letter_vecs.shape[1])\n",
    "    \n",
    "    full_str = '#' + ngram_str + '.'\n",
    "    \n",
    "    for il, l in enumerate(full_str[:-(window-1)]):\n",
    "        trivec = letter_vecs[alph.find(full_str[il]), :]\n",
    "        for c3 in range(1, window):\n",
    "            trivec = trivec * np.roll(letter_vecs[alph.find(full_str[il+c3]), :], c3)\n",
    "            \n",
    "        vec += trivec\n",
    "    return 2* (vec + 0.1*(np.random.rand(letter_vecs.shape[1])-0.5) < 0) - 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alph = 'abcdefghijklmnopqrstuvwxyz#.'\n",
    "\n",
    "N = 1000\n",
    "D = len(alph)\n",
    "n_steps = 50\n",
    "letter_vecs = 2 * (np.random.randn(D, N) < 0) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(ngram_strs, encode):\n",
    "    psi = np.zeros(N)\n",
    "    for i in range(len(ngram_strs)):\n",
    "        psi += encode(ngram_strs[i], letter_vecs, alph)\n",
    "    return psi\n",
    "\n",
    "def predict_coefs(n_steps, ngram_str, bound_vec):\n",
    "    l_states = np.random.randn(len(ngram_str)+2, N)\n",
    "    for i in range(1,l_states.shape[0]-1):\n",
    "        l_states[i] = np.dot(letter_vecs.T, np.dot(l_states[i], letter_vecs.T))\n",
    "        l_states[i] = l_states[i]/norm(l_states[i])\n",
    "\n",
    "    l_states[0] = letter_vecs[alph.find('#'), :]\n",
    "    l_states[l_states.shape[0]-1] = letter_vecs[alph.find('.'), :]\n",
    "\n",
    "    l_coef_hists = np.zeros((n_steps, l_states.shape[0], D))\n",
    "    for i in range(n_steps):\n",
    "        for j in range(1,l_states.shape[0]-1):\n",
    "            l_coef_hists[i, j, :] = np.dot(letter_vecs, l_states[j])\n",
    "\n",
    "            ## Need to make sure that the largest absolute value is always positive,\n",
    "            ## because the unbinding inference can flip 2 and be the same\n",
    "\n",
    "            mxjidx = np.argmax(np.abs(l_coef_hists[i, j, :]))\n",
    "            l_states[j] *= np.sign(l_coef_hists[i, j, mxjidx])\n",
    "            if j == 1:\n",
    "                ljd = (np.roll(bound_vec * l_states[j-1] * np.roll(l_states[j+1], 2), -1) +\n",
    "                  bound_vec * np.roll(l_states[j+1], 1) * np.roll(l_states[j+2], 2))\n",
    "            elif 1 < j < l_states.shape[0]-2:\n",
    "                ljd = (np.roll(bound_vec * l_states[j-2] * np.roll(l_states[j-1], 1), -2) +\n",
    "                    np.roll(bound_vec * l_states[j-1] * np.roll(l_states[j+1], 2), -1) +\n",
    "                        bound_vec * np.roll(l_states[j+1], 1) * np.roll(l_states[j+2], 2))\n",
    "            else:\n",
    "                ljd = (np.roll(bound_vec * l_states[j-1] * np.roll(l_states[j+1], 2), -1) +\n",
    "                       np.roll(bound_vec * l_states[j-2] * np.roll(l_states[j-1], 1), -2))\n",
    "\n",
    "            l_states[j] = np.dot(letter_vecs.T, np.dot(ljd, letter_vecs.T))\n",
    "            l_states[j] = l_states[j]/norm(l_states[j])\n",
    "    return l_states, l_coef_hists\n",
    "\n",
    "def graph(n_steps, l_coef_hists):\n",
    "    figure(figsize=(10,3))\n",
    "    cols = get_cmap('copper', n_steps)\n",
    "    for i in range(n_steps):\n",
    "        for j in range(1,l_coef_hists.shape[1]-1):\n",
    "            subplot(130+j)\n",
    "            plot(abs(l_coef_hists[i,j,:]), lw=3, c=cols(i))\n",
    "    \n",
    "def predict(l_coef_hists):\n",
    "    prediction = \"\"\n",
    "    for i in range(1,l_coef_hists.shape[1]-1):\n",
    "        prediction += alph[np.argmax(abs(l_coef_hists[-1,i,:]))]\n",
    "    print prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing noise tolerance\n",
    "# does this mean that bound vec specific to a word \n",
    "# has the same dot product for every correct pair????\n",
    "#ngram_str = 'banana'#'the'\n",
    "ngram_str = 'the'\n",
    "train_set = ['the', 'fam', 'sailboat', 'nuclear', 'compulsive', 'whatisthis']\n",
    "ngram_vecs = [ngram_encode_cl(verb, letter_vecs, alph) for verb in train_set]\n",
    "psi = train(train_set, ngram_encode)\n",
    "similarities = np.zeros(len(train_set)**2)\n",
    "\n",
    "#figure()\n",
    "for i in range(len(ngram_vecs)):\n",
    "    bound_vec = psi*ngram_vecs[i]\n",
    "    bound_vec = 2* (bound_vec + 0.1*(np.random.rand(letter_vecs.shape[1])-0.5) < 0) - 1\n",
    "    #print bound_vec\n",
    "    for j in range(len(ngram_vecs)):\n",
    "        #if i == j:\n",
    "        similarities[i+j] = bound_vec.dot(ngram_vecs[j])\n",
    "        #print (train_set[i], train_set[j], similarities[i+j])\n",
    "#plot(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banana\n"
     ]
    }
   ],
   "source": [
    "#ngram_str = 'banana'#'the'\n",
    "ngram_str = 'banana'\n",
    "psi = train(['banana'], ngram_encode)\n",
    "bound_vec = psi\n",
    "l_states, l_coef_hists = predict_coefs(n_steps, ngram_str, bound_vec)\n",
    "#graph(n_steps, l_coef_hists)\n",
    "predict(l_coef_hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
