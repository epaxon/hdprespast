{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from pylab import *\n",
    "import scipy\n",
    "import time\n",
    "import utils\n",
    "import hrr_utils\n",
    "\n",
    "#import imnet_utils as imut\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA, FastICA, TruncatedSVD, NMF\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update({'axes.titlesize': 'xx-large'})\n",
    "plt.rcParams.update({'axes.labelsize': 'xx-large'})\n",
    "plt.rcParams.update({'xtick.labelsize': 'x-large', 'ytick.labelsize': 'x-large'})\n",
    "plt.rcParams.update({'legend.fontsize': 'x-large'})\n",
    "plt.rcParams.update({'text.usetex': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_encode(ngram_str, letter_vecs, alph):\n",
    "    vec = np.zeros(letter_vecs.shape[1])\n",
    "    \n",
    "    full_str = '#' + ngram_str + '.'\n",
    "    \n",
    "    \n",
    "    for il, l in enumerate(full_str[:-2]):\n",
    "        trivec = letter_vecs[alph.find(full_str[il]), :]\n",
    "        for c3 in range(1, 3):\n",
    "            trivec = trivec * np.roll(letter_vecs[alph.find(full_str[il+c3]), :], c3)\n",
    "            \n",
    "        vec += trivec\n",
    "    return vec\n",
    "\n",
    "def ngram_encode_cl(ngram_str, letter_vecs, alph):\n",
    "    vec = ngram_encode(ngram_str, letter_vecs, alph)\n",
    "    \n",
    "    return 2* (vec + 0.1*(np.random.rand(letter_vecs.shape[1])-0.5) > 0) - 1\n",
    "\n",
    "def bind(past, present):\n",
    "    return np.multiply(past, present)\n",
    "\n",
    "def reg_bind(past, present):\n",
    "    pred = np.multiply(tv, present)\n",
    "    return ((N-sim(pred, past))/float(N)) * np.multiply(past, present)\n",
    "\n",
    "def bind_diff(past2, present1, present2):\n",
    "    return np.multiply(present1, past2-present2)\n",
    "\n",
    "def reg_bind_diff(past2, present1, present2):\n",
    "    pred = np.multiply(tv, present1) + present2\n",
    "    return ((N-sim(pred, past2))/float(N)) * np.multiply(past2-present2, present1)\n",
    "\n",
    "def closed_bind(past, present):\n",
    "    return np.dot(np.linalg.pinv(present), past)\n",
    "\n",
    "def sim(x, y):\n",
    "    if len(x.shape) == 1 or len(y.shape)==1:\n",
    "        return np.dot(x, y)\n",
    "    return np.sum(np.multiply(x, y), axis=1)\n",
    "\n",
    "def state_setup(state_length, letter_vecs, N, D, n_steps):\n",
    "    states = []\n",
    "    coef_hists = []\n",
    "    \n",
    "    for i in range(state_length):\n",
    "        states.append(np.random.randn(N))\n",
    "    \n",
    "    for i in range(1, state_length-1):\n",
    "        states[i] = np.dot(letter_vecs.T, np.dot(states[i], letter_vecs.T))\n",
    "\n",
    "    for i in range(1, state_length-1):\n",
    "        states[i] = states[i]/norm(states[i])\n",
    "\n",
    "    states[0] = letter_vecs[alph.find('#'), :]\n",
    "    states[state_length-1] = letter_vecs[alph.find('.'), :]\n",
    "    \n",
    "    for i in range(1, state_length-1):\n",
    "        coef_hists.append(np.zeros((n_steps, D)))\n",
    "    \n",
    "    return states, coef_hists\n",
    "\n",
    "# EXPLAIN AWAY, PAST PRESENT TENSE DIFFERENCE\n",
    "def explain_away_iterate(bound_vec, states, coef_hists, state_length, letter_vecs, N, D, n_steps):\n",
    "    th_vec = bound_vec.copy()\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        for j in range(1, state_length-1):\n",
    "            coef_hists[j-1][i, :] = np.dot(letter_vecs, states[j])\n",
    "            ## Need to make sure that the largest absolute value is always positive,\n",
    "            ## because the unbinding inference can flip 2 and be the same\n",
    "\n",
    "            #mxidx = np.argmax(np.abs(coef_hists[j-1][i,:]))\n",
    "            #states[j] *= np.sign(coef_hists[j-1][i, mxidx])\n",
    "            \n",
    "            if j == 1:\n",
    "                ljd = (np.roll(th_vec * states[0] * np.roll(states[j+1], 2), -1) +\n",
    "                  th_vec * np.roll(states[j+1], 1) * np.roll(states[j+2], 2)) / 2\n",
    "            elif 1 < j < state_length-2:\n",
    "                ljd = (np.roll(th_vec * states[j-2] * np.roll(states[j-1], 1), -2) +\n",
    "                    np.roll(th_vec * states[j-1] * np.roll(states[j+1], 2), -1) +\n",
    "                      th_vec * np.roll(states[j+1], 1) * np.roll(states[j+2], 2)) / 3\n",
    "            else:\n",
    "                ljd = (np.roll(th_vec * states[j-1] * np.roll(states[j+1], 2), -1) +\n",
    "                   np.roll(th_vec * states[j-2] * np.roll(states[j-1], 1), -2)) / 2\n",
    "\n",
    "            states[j] = np.dot(letter_vecs.T, np.dot(ljd, letter_vecs.T)/N) + 1.0*states[j]\n",
    "            #states[j] = states[j]/norm(states[j])\n",
    "            \n",
    "        bv = states[0] * np.roll(states[1],1) * np.roll(states[2],2) \n",
    "              \n",
    "        for j in range(1, state_length-2):\n",
    "            bv += states[j] * np.roll(states[j+1],1) * np.roll(states[j+2],2) \n",
    "         \n",
    "        #bv = 2*(bv > 0) - 1\n",
    "\n",
    "        th_vec = bound_vec - bv\n",
    "\n",
    "    return states, coef_hists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alph = 'abcdefghijklmnopqrstuvwxyz#.'\n",
    "\n",
    "N = 100000\n",
    "D = len(alph)\n",
    "\n",
    "letter_vecs = 2 * (np.random.randn(D, N) < 0) - 1\n",
    "letter_vecs1 = 2 * (np.random.randn(D, N) < 0) - 1\n",
    "\n",
    "word_key = 2 * (np.random.randn(N) < 0) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pres, reg_past, reg_freq = utils.GetRegularVerbs(frequency=1)\n",
    "irreg_pres, irreg_past, irreg_freq = utils.GetIrregularVerbs(frequency=1)\n",
    "\n",
    "cut = 115\n",
    "present_strs = irreg_pres[:cut//2] + reg_pres[:cut]\n",
    "past_strs = irreg_past[:cut//2] + reg_past[:cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181, ['arose', 'awoke', 'was', 'bore', 'beat', 'became', 'begot', 'began', 'bent', 'bereaved', 'besought', 'bet', 'bade', 'bade', 'bound', 'bit', 'bled', 'blew', 'broke', 'bred', 'brought', 'broadcast', 'built', 'burnt', 'burst', 'bust', 'bought', 'could', 'cast', 'caught', 'chose', 'cleft', 'clung', 'clothed', 'came', 'cost', 'crept', 'cut', 'dealt', 'dug', 'did', 'drew', 'dreamt', 'drank', 'drove', 'dwelt', 'ate', 'fell', 'fed', 'felt', 'fought', 'found', 'fled', 'flung', 'forbad', 'forecast', 'forgot', 'alighted', 'blessed', 'crowed', 'gelded', 'hewed', 'made', 'saw', 'gat', 'knew', 'went', 'took', 'said', 'thought', 'came', 'found', 'gave', 'let', 'wanted', 'looked', 'told', 'kept', 'became', 'seemed', 'used', 'provided', 'helped', 'liked', 'showed', 'felt', 'believed', 'put', 'left', 'worked', 'needed', 'brought', 'meant', 'lived', 'heard', 'held', 'turned', 'met', 'remembered', 'understood', 'tried', 'called', 'moved', 'paid', 'considered', 'ran', 'asked', 'appeared', 'talked', 'included', 'played', 'spoke', 'expected', 'stood', 'continued', 'determined', 'served', 'wrote', 'started', 'followed', 'stayed', 'supposed', 'stopped', 'remained', 'set', 'reached', 'developed', 'read', 'added', 'carried', 'cut', 'wished', 'required', 'began', 'learned', 'prevented', 'built', 'increased', 'led', 'waited', 'indicated', 'entered', 'changed', 'received', 'cared', 'acted', 'placed', 'returned', 'produced', 'sent', 'accepted', 'allowed', 'realized', 'sought', 'bought', 'hoped', 'offered', 'fell', 'permitted', 'sat', 'walked', 'broke', 'passed', 'assumed', 'explained', 'happened', 'joined', 'recognized', 'reduced', 'grew', 'imagined', 'killed', 'ran', 'walked', 'watched', 'caught', 'sat', 'jumped', 'lumped', 'bumped', 'thought'])\n"
     ]
    }
   ],
   "source": [
    "present_strs += ['run', 'walk', 'watch', 'catch', 'sit', 'jump', 'lump', 'bump', 'think']\n",
    "past_strs += ['ran', 'walked', 'watched', 'caught', 'sat', 'jumped', 'lumped', 'bumped', 'thought']\n",
    "\n",
    "print (len(past_strs), past_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bound_vec = np.zeros(N)\n",
    "reg_bound_vec = np.zeros(N)\n",
    "\n",
    "\n",
    "for i in range(len(past_strs)):\n",
    "    past = ngram_encode_cl(past_strs[i], letter_vecs, alph)\n",
    "    pres = ngram_encode_cl(present_strs[i], letter_vecs, alph)\n",
    "    past1 = ngram_encode_cl(past_strs[i], letter_vecs1, alph)\n",
    "    pres1 = ngram_encode_cl(present_strs[i], letter_vecs1, alph)\n",
    "    \n",
    "    bound_diff = pres * (past1 - pres1)\n",
    "    pred = reg_bound_vec*pres + pres1\n",
    "\n",
    "    bound_vec += bound_diff\n",
    "    reg_bound_vec += ((N-sim(pred, past1))/float(N)) * bound_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # predict a word in bound_vec\n",
    "# # regularized bind\n",
    "# seen = 'seen-'\n",
    "# word = 'arise'\n",
    "# word_length = 5\n",
    "# word_vec = ngram_encode_cl(word, letter_vecs, alph)\n",
    "# word_vec1 = ngram_encode_cl(word, letter_vecs1, alph)\n",
    "\n",
    "# pred_vec = word_vec * bound_vec + word_vec1\n",
    "\n",
    "# predict a word not bound_vec\n",
    "# regularized bind\n",
    "seen = 'unseen-'\n",
    "word = 'pump'\n",
    "word_length = 6\n",
    "word_vec = ngram_encode_cl(word, letter_vecs, alph)\n",
    "word_vec1 = ngram_encode_cl(word, letter_vecs1, alph)\n",
    "\n",
    "pred_vec = word_vec * bound_vec + word_vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 80\n",
    "states, coef_hists = state_setup(word_length+2, letter_vecs1, N, D, n_steps)\n",
    "states, coef_hists = explain_away_iterate(pred_vec, states, coef_hists, word_length+2, letter_vecs1, N, D, n_steps)\n",
    "#states, coef_hists = iterate(pred_vec, states, coef_hists, word_length+2, letter_vecs1, N, D, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pumssd\n"
     ]
    }
   ],
   "source": [
    "def resplot(word_length, states, coef_hists, title, N, nsteps, reg):\n",
    "    pred = ''\n",
    "    alphis = []\n",
    "    for i in range(len(coef_hists)):\n",
    "        alphis.append(np.argmax(np.abs(coef_hists[i][-1,:])))\n",
    "        pred += alph[alphis[i]]\n",
    "\n",
    "    print pred\n",
    "    \n",
    "#     rows = 1\n",
    "#     columns = word_length\n",
    "\n",
    "#     fig, axes = plt.subplots(rows, columns, sharex='all', squeeze=True, figsize=(25,5))\n",
    "#     cols = get_cmap('copper', min(500,n_steps))\n",
    "#     x = np.linspace(0,len(alph)-2,len(alph)-2)\n",
    "#     labels = list(alph)\n",
    "#     plt.xticks(x, labels)\n",
    "    \n",
    "#     for j in range(word_length):\n",
    "#         for i in range(min(50,n_steps)):\n",
    "#             # graphing the max positive at every iteration is not intuitive, since we should\n",
    "#             # be focusing on how our predicted letter's probability increases over time\n",
    "#             coef_hists[j][i,alphis[j]] = np.abs(coef_hists[j][i,alphis[j]])\n",
    "#             axes[j].plot(coef_hists[j][i,:], lw=1.7, c=cols(i))\n",
    "            \n",
    "#         step, alphi = np.unravel_index(coef_hists[j].argmax(), coef_hists[j].shape)\n",
    "#         axes[j].plot(alphi, coef_hists[j][step, alphi], '+')\n",
    "    \n",
    "#     plt.savefig('figures/'+title+pred+'-N='+str(N)+'-steps='+str(nsteps)+'-reg='+reg+'.svg')\n",
    "    \n",
    "resplot(word_length, states, list(coef_hists), word, N, n_steps, 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
