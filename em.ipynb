{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to work with phonemes: http://www.nltk.org/_modules/nltk/corpus/reader/cmudict.html\n",
    "# from nltk.corpus import brown\n",
    "# from nltk.corpus import gutenberg\n",
    "# from nltk.corpus import cmudict\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from utils import random_idx\n",
    "from utils import utils\n",
    "from utils import lang_vectors_utils as lvu\n",
    "\n",
    "k = 5000\n",
    "N = 10000\n",
    "cluster_sizes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "ordered = 1\n",
    "alphabet = string.lowercase + ' '\n",
    "\n",
    "file_ids = nltk.corpus.gutenberg.fileids()\n",
    "num_iterations = 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_unigrams(vocab, text):\n",
    "    #handle case of 1 letter words:\n",
    "    copy = text.split(\" \")\n",
    "    for word in copy:\n",
    "        if len(word) == 1:\n",
    "            if word in vocab[1].keys():\n",
    "                vocab[1][word] += 1\n",
    "            #do something about discovering unigrams\n",
    "            \n",
    "# need to fix bc using nltk now\n",
    "def tuples_to_text(tuples, text):\n",
    "    texted = [] # \"\"\n",
    "    for tup in tuples:\n",
    "        word = text[tup[0]:tup[1]]\n",
    "        texted.append(word) # += \" \" + word\n",
    "    return texted\n",
    "\n",
    "# unioned windowing is way too slow.\n",
    "# empty strings added to processed_indices. idk why\n",
    "def dict_explain_away(vocab,max_length,text):\n",
    "    print len(text)\n",
    "    # text[:] does not make a new copy\n",
    "    duplicate = \"%s\" % text\n",
    "    #print id(text)\n",
    "    #print id(duplicate)\n",
    "    # make a list of disjoint tuples of (start_index, end_index)\n",
    "    preprocessed_indices = []\n",
    "\n",
    "    for i in range(len(vocab)-1, -1,-1):\n",
    "        for key in vocab[i].keys():\n",
    "            starts = [match.start() for match in re.finditer(re.escape(key), duplicate)]\n",
    "            for start in starts:\n",
    "                mask = \" \"*len(key)\n",
    "                duplicate = duplicate[:start] + mask + duplicate[start+len(key):]\n",
    "                preprocessed_indices.append([start, start + len(key)])\n",
    "    processed_indices = [[pair[0],pair[1]] for pair in preprocessed_indices if pair[0] < pair[1]]\n",
    "\n",
    "    return sorted(processed_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_iterations):\n",
    "    words = [word.encode(\"ascii\").lower() for word in nltk.corpus.gutenberg.words(file_ids[i])]\n",
    "    # preserves indices for testing\n",
    "    words = [word if word.isalpha() else None for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "brown_tagged = brown.tagged_words(tagset='universal')\n",
    "verb_tagged = [(a, b) for (a, b) in brown_tagged if str(b) == \"VERB\"]\n",
    "verb_fd = nltk.FreqDist(word for (word, tag) in verb_tagged)\n",
    "print len(verb_fd)\n",
    "print verb_fd.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a threshold > 0 so that if a word really is new \n",
    "# (not connected to any known roots), then add to vocab\n",
    "def hard_e_step(word,vocab,lv,threshold=0):\n",
    "    cluster_size = len(vocab)\n",
    "    # if word is > max vocab\n",
    "    if len(word) < cluster_size:\n",
    "        cluster_size = len(word)\n",
    "    potential_words = []\n",
    "    word_vector = random_idx.id_vector(N, word, alphabet, lv, ordered)\n",
    "\n",
    "    for i in range(cluster_size-1, -1,-1):\n",
    "        for key in vocab[i].keys():\n",
    "            key_vector = random_idx.id_vector(N, key, alphabet, lv, ordered)\n",
    "            similarity = np.dot(np.transpose(word_vector[0]), key_vector[0])\n",
    "            if similarity > threshold:\n",
    "                potential_words.append([similarity, key])\n",
    "    return potential_words\n",
    "\n",
    "\n",
    "# records frequencies and word(s) in vocab\n",
    "def hard_m_step(word,potential_words,vocab):\n",
    "    sorted(potential_words)\n",
    "    if not potential_words:\n",
    "        vocab[len(word)-1][word] = 1\n",
    "        return word\n",
    "    potential_word = potential_words[len(potential_words)-1]\n",
    "    #print potential_word\n",
    "    # need to add vocab since pulling words out can slice words in a way\n",
    "    # that introduces new keys (beginnings/endings)\n",
    "    if potential_word[1] not in vocab[len(potential_word)-1]:\n",
    "        vocab[len(potential_word)-1][potential_word[1]] = 1\n",
    "    else:\n",
    "        vocab[len(potential_word)-1][potential_word[1]] += 1\n",
    "    return potential_word\n",
    "\n",
    "\n",
    "def hard_em_discover_words(processed_indices,text,vocab,lv):\n",
    "    # find the left over intervals\n",
    "    undiscovered = []\n",
    "    # undiscovered at beginning of text\n",
    "    if processed_indices[0][0] > 0:\n",
    "        undiscovered.append([0,processed_indices[0][0]])\n",
    "    for i in range(0,len(processed_indices)-1):\n",
    "        start = processed_indices[i][1]\n",
    "        end = processed_indices[i+1][0]-1\n",
    "        if start < end:\n",
    "            undiscovered.append([start, end])\n",
    "            #print text[start:end]\n",
    "    # undiscovered at end of text\n",
    "    if processed_indices[len(processed_indices)-1][1] < len(text)-1:\n",
    "        undiscovered.append([processed_indices[len(processed_indices)-1][1],len(text)-1])\n",
    "\n",
    "    processed = []\n",
    "    for pair in undiscovered:\n",
    "        word = text[pair[0]:pair[1]]\n",
    "        potential_words = hard_e_step(word,vocab,lv)\n",
    "        potential_word = hard_m_step(word,potential_words,vocab)\n",
    "        processed.append([pair,potential_word])\n",
    "    return processed\n",
    "\n",
    "\n",
    "def record_results(text, processed_indices,discovered_words,output_url=\"output/explain_away_results.txt\"):\n",
    "    fwrite = open(output_url,\"w\")\n",
    "    discovered_index = 0\n",
    "    # discovered at beginning of text\n",
    "    if processed_indices[0][0] > 0:\n",
    "        fwrite.write(discovered_words[0][1][1] + \" \")\n",
    "        discovered_index += 1\n",
    "    for i in range(0,len(processed_indices)-1):\n",
    "        fwrite.write(text[processed_indices[i][0]:processed_indices[i][1]] + \" \")\n",
    "        start = processed_indices[i][1]\n",
    "        end = processed_indices[i+1][0]-1\n",
    "        if start < end:\n",
    "            fwrite.write(discovered_words[discovered_index][1][1] + \" \")\n",
    "            discovered_index += 1\n",
    "    # discovered at end of text\n",
    "    if processed_indices[len(processed_indices)-1][1] < len(text)-1:\n",
    "        fwrite.write(discovered_words[len(discovered_words)-1][1][1])\n",
    "    fwrite.close()\n",
    "\n",
    "def seed():\n",
    "    filepath = \"preprocessed_texts/english/alice-only-spaced.txt\"\n",
    "    #lvu.initialize()\n",
    "    lv, lang_vectors, n_gram_frequencies = lvu.initialize_from_file()\n",
    "    vocab_vec, max_word_length = lvu.vocab_vector(lv, lang_vectors)\n",
    "    vocab = lvu.vocab(max_word_length)\n",
    "\n",
    "    filepath = \"preprocessed_texts/english/alice-only-stream.txt\"#a_christmas_carol.txt\"\n",
    "    text = read_file(filepath)\n",
    "    processed_indices = dict_explain_away(vocab,max_word_length,text)\n",
    "\n",
    "    processed = tuples_to_text(processed_indices, text)\n",
    "\n",
    "    fwrite = open(\"intermediate/processed_dict_explain_away_results.txt\",\"w\")\n",
    "    fwrite.write(processed)\n",
    "    fwrite.close()\n",
    "\n",
    "    # now for the em\n",
    "    # not necessary in seeding phase. \n",
    "    discovered_words = hard_em_discover_words(processed_indices, text, vocab, lv)\n",
    "    record_results(text, processed_indices,discovered_words,\"output/explain_away_results.txt\")\n",
    "\n",
    "    # save data to file\n",
    "    lvu.write_data_structures([lv, lang_vectors, n_gram_frequencies, vocab_vec, vocab], \\\n",
    "        [\"intermediate/lookup_lv\", \"intermediate/lookup_lang_vectors\", \\\n",
    "        \"intermediate/lookup_n_gram_frequencies\", \"intermediate/lookup_vocab_vec\", \\\n",
    "        \"intermediate/lookup_vocab\"])\n",
    "\n",
    "seed()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
