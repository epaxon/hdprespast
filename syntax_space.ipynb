{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "from stop_words import get_stop_words\n",
    "#from scipy import sparse\n",
    "from scipy import spatial\n",
    "import gensim\n",
    "import utils\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "import pickle\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from utils import random_idx\n",
    "from utils import utils\n",
    "from utils import lang_vectors_utils as lvu\n",
    "from utils import documents\n",
    "%matplotlib inline\n",
    "\n",
    "num_topics = 50\n",
    "passes = 20\n",
    "#topn = 10\n",
    "k = 5000\n",
    "N = 10000\n",
    "# cluster_sizes is mapping to n-gram size\n",
    "# cluster_sz in random_idx referring to specific element (int) in cluster_sizes, array\n",
    "cluster_sizes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "ordered = 1\n",
    "#assuming this is the alphabet bc of precedent in generate_text.py\n",
    "#alph = 'abc' \n",
    "alphabet = string.lowercase + ' '\n",
    "RI_letters = random_idx.generate_letter_id_vectors(N, k, alphabet)\n",
    "# words should be unique? (such as permutations)\n",
    "# number of words added in a syntax vector\n",
    "syntax_granularities = [5,10,100,200]\n",
    "lower_n_cutoff = .85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_documents(documents):\n",
    "    vectors = {}\n",
    "    for doc in documents:\n",
    "        for word in doc:\n",
    "            vectors[word] = random_idx.id_vector(N, word, alphabet, RI_letters)\n",
    "    return vectors\n",
    "\n",
    "def similarity_matrix(dictionary,id_to_word):\n",
    "    \"\"\"\n",
    "    calculate cosine similarity of every word\n",
    "    \"\"\"\n",
    "    num_words = len(id_to_word)\n",
    "    sm = np.zeros((num_words,num_words))\n",
    "    cutoff_index = int(lower_n_cutoff*num_words)\n",
    "    cutoff = float(\"inf\")\n",
    "\n",
    "    for row in range(0,num_words):\n",
    "        for col in range(0,num_words):\n",
    "            sm[row][col] = np.dot(np.transpose(dictionary[id_to_word[row]][0]), dictionary[id_to_word[col]][0])\n",
    "        # find the 85% smallest element in array\n",
    "        cutoff = min(cutoff, np.partition(sm[row], cutoff_index)[cutoff_index])\n",
    "    return sm, cutoff\n",
    "\n",
    "def syntax_space(similarity_matrix, vectorized_dictionary, id_to_word, cutoff):\n",
    "    \"\"\"\n",
    "    number of granularities of words encoded per syntax vector x number of tokens x N\n",
    "    index in numpy array on axis \"number of tokens\" equivalent to token id\n",
    "    http://stackoverflow.com/questions/6910641/how-to-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "    \"\"\"\n",
    "    num_tokens = len(vectorized_dictionary.keys())\n",
    "    print \"num_tokens\"\n",
    "    matrices = np.zeros((len(syntax_granularities), num_tokens, N))\n",
    "    print \"made matrices\"\n",
    "    print matrices.shape\n",
    "    for gran_i in range(len(syntax_granularities)):\n",
    "        print gran_i\n",
    "        for row in range(num_tokens):\n",
    "            granularity = min(syntax_granularities[gran_i],num_tokens)\n",
    "            for dot in similarity_matrix[row]:\n",
    "                if dot >= cutoff:\n",
    "                    matrices[gran_i][row] = np.add(matrices[gran_i][row],vectorized_dictionary[id_to_word[row]][0])\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run tests\n",
    "def accuracy_syntax(test_files, test_documents, id_to_word, vectorized_dictionary, syntaxed_space, syntax_cutoff, syntax_granularities):\n",
    "    accuracy_per_granularity = np.zeros((len(test_files),len(syntax_granularities)))\n",
    "    \"\"\"\n",
    "    for i in range(len(test_files)):\n",
    "        test_tokens = test_documents[i]\n",
    "        for gran_i in range(len(syntax_granularities)):\n",
    "            for test_token in test_tokens:\n",
    "                #number of granularities x number of tokens x N\n",
    "                #N, dictionary[i], alphabet, RI_letters\n",
    "                prediction_vector = np.dot(syntaxed_space[gran_i], np.transpose(random_idx.id_vector(N, test_token, alphabet, RI_letters)))\n",
    "                prediction_id = np.argmax(prediction_vector)\n",
    "                # account for new tokens and similarity not identity\n",
    "                # >= bottom 85% cutoff for same vector group from similarity matrix\n",
    "                if prediction_vector[prediction_id] >= syntax_cutoff:\n",
    "                    accuracy_per_granularity[i][gran_i]+=1\n",
    "        accuracy_per_granularity[i] /= len(test_tokens)\n",
    "    \"\"\"\n",
    "    return accuracy_per_granularity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graphing_3d():\n",
    "    # each vector maps to the test_token. dots graphs but mappings can printed\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    xs =[1,2,3,4,5,6,7,8,9,10]\n",
    "    ys =[5,6,2,3,13,4,1,2,4,8]\n",
    "    zs =[2,3,3,3,5,7,9,11,9,10]\n",
    "\n",
    "    xt =[-1,-2,-3,-4,-5,-6,-7,8,-9,-10]\n",
    "    yt =[-5,-6,-2,-3,-13,-4,-1,2,-4,-8]\n",
    "    zt =[-2,-3,-3,-3,-5,-7,9,-11,-9,-10]\n",
    "\n",
    "    ax.scatter(xs, ys, zs, c='r', marker='o')\n",
    "    ax.scatter(xt, yt, zt, c='b', marker='^')\n",
    "\n",
    "    ax.set_xlabel('X Label')\n",
    "    ax.set_ylabel('Y Label')\n",
    "    ax.set_zlabel('Z Label')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def graphing_2d(test_files, syntax_granularities, accuracy_per_granularity):\n",
    "    for i in range(accuracy_per_granularity.shape[0]):\n",
    "        print test_files[i]\n",
    "        print len(syntax_granularities)\n",
    "        print accuracy_per_granularity[i].shape\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(syntax_granularities, accuracy_per_granularity[i], c='r', marker='o')\n",
    "\n",
    "        ax.set_xlabel('granularity')\n",
    "        ax.set_ylabel('accuracy %')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "raw_path = \"raw_texts/texts_english/\"\n",
    "preprocessed_path = \"preprocessed_texts/english/\"\n",
    "training_preprocessed_path = \"preprocessed_texts/english/with_spaces/\"\n",
    "\n",
    "training_files = [\"a_christmas_carol.txt\", \"alice_in_wonderland.txt\"]\n",
    "# this is for testing accuracy against the \n",
    "# actual stream that will be the test input\n",
    "test_files = [\"hamlet_english.txt\", \"percy_addleshaw.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_doc_set = documents.create_doc_set(training_preprocessed_path, training_files)\n",
    "test_doc_set = documents.create_doc_set(preprocessed_path, test_files)\n",
    "\n",
    "training_documents = [doc.split() for doc in training_doc_set]\n",
    "test_documents = [doc.split() for doc in test_doc_set]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorized_dictionary = vectorize_documents(training_documents)\n",
    "id_to_word = {}\n",
    "words = vectorized_dictionary.keys()\n",
    "for i in range(len(words)):\n",
    "    id_to_word[i] = words[i]\n",
    "fwrite = open(\"vectorized_dictionary\", \"w\")\n",
    "fwrite1 = open(\"id_to_word\", \"w\")\n",
    "pickle.dump(vectorized_dictionary, fwrite)\n",
    "pickle.dump(id_to_word, fwrite1)\n",
    "fwrite.close()\n",
    "fwrite1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+04  -4.00000000e+00  -7.60000000e+01 ...,  -1.60000000e+01\n",
      "    1.40000000e+02  -8.80000000e+01]\n",
      " [ -4.00000000e+00   1.00000000e+04  -4.00000000e+01 ...,  -1.04000000e+02\n",
      "   -8.00000000e+01   0.00000000e+00]\n",
      " [ -7.60000000e+01  -4.00000000e+01   1.00000000e+04 ...,  -6.40000000e+01\n",
      "   -1.60000000e+01   1.64000000e+02]\n",
      " ..., \n",
      " [ -1.60000000e+01  -1.04000000e+02  -6.40000000e+01 ...,   1.00000000e+04\n",
      "   -1.40000000e+02   2.40000000e+01]\n",
      " [  1.40000000e+02  -8.00000000e+01  -1.60000000e+01 ...,  -1.40000000e+02\n",
      "    1.00000000e+04  -2.40000000e+01]\n",
      " [ -8.80000000e+01   0.00000000e+00   1.64000000e+02 ...,   2.40000000e+01\n",
      "   -2.40000000e+01   1.00000000e+04]]\n"
     ]
    }
   ],
   "source": [
    "fread = open(\"vectorized_dictionary\", \"r\")\n",
    "fread1 = open(\"id_to_word\", \"r\")\n",
    "vectorized_dictionary = pickle.load(fread)\n",
    "id_to_word = pickle.load(fread1)\n",
    "fread.close()\n",
    "fread1.close()\n",
    "# similarity matrix\n",
    "sm, syntax_cutoff = similarity_matrix(vectorized_dictionary, id_to_word)\n",
    "print sm\n",
    "fwrite = open(\"sm\", \"w\")\n",
    "fwrite1 = open(\"syntax_cutoff\", \"w\")\n",
    "pickle.dump(sm, fwrite)\n",
    "pickle.dump(syntax_cutoff, fwrite1)\n",
    "fwrite.close()\n",
    "fwrite1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens\n",
      "made matrices\n",
      "(4, 5730, 10000)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "fread = open(\"vectorized_dictionary\", \"r\")\n",
    "fread1 = open(\"id_to_word\", \"r\")\n",
    "fread2 = open(\"sm\", \"r\")\n",
    "fread3 = open(\"syntax_cutoff\", \"r\")\n",
    "vectorized_dictionary = pickle.load(fread)\n",
    "id_to_word = pickle.load(fread1)\n",
    "sm = pickle.load(fread2)\n",
    "syntax_cutoff = pickle.load(fread3)\n",
    "fread.close()\n",
    "fread1.close()\n",
    "fread2.close()\n",
    "fread3.close()\n",
    "\n",
    "syntaxed_space = syntax_space(sm, vectorized_dictionary, id_to_word, syntax_cutoff)\n",
    "fwrite = open(\"syntaxed_space\", \"w\")\n",
    "pickle.dump(syntaxed_space, fwrite)\n",
    "fwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# since humza is already doing the syntax space I want (in his case the meaning space)\n",
    "# my syntax space isn't that necessary for tenses, but mainly words that look similar. another way for syntax?\n",
    "fread = open(\"sm\", \"r\")\n",
    "fread1 = open(\"syntax_cutoff\", \"r\")\n",
    "fread2 = open(\"vectorized_dictionary\", \"r\")\n",
    "fread3 = open(\"id_to_word\", \"r\")\n",
    "fread4 = open(\"syntaxed_space\", \"r\")\n",
    "sm = pickle.load(fread)\n",
    "syntax_cutoff = pickle.load(fread1)\n",
    "vectorized_dictionary = pickle.load(fread2)\n",
    "id_to_word = pickle.load(fread3)\n",
    "syntaxed_space = pickle.load(fread4)\n",
    "fread.close()\n",
    "fread1.close()\n",
    "fread2.close()\n",
    "fread3.close()\n",
    "fread4.close()\n",
    "accuracy_per_granularity = accuracy_syntax(test_files, test_documents, id_to_word, vectorized_dictionary, syntaxed_space, syntax_cutoff, syntax_granularities)\n",
    "fwrite = open(\"accuracy_per_granularity\",\"w\")\n",
    "pickle.dump(syntaxed_space, fwrite)\n",
    "fwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph the accuracy per granularity\n",
    "fread = open(\"accuracy_per_granularity\",\"r\")\n",
    "accuracy_per_granularity = pickle.load(fread)\n",
    "fread.close()\n",
    "print accuracy_per_granularity.shape\n",
    "# actual shape is (4, 5730, 10000)\n",
    "# supposed to be: (num_test_files, num_granularities) = (2,4)\n",
    "# graphing function itself is fine\n",
    "#graphing_2d(test_files, syntax_granularities, accuracy_per_granularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
