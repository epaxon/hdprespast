\section{Discussion}
\subsection{Why Not Neural Nets?}
Neural networks are characterized by extraordinarily long training time and exponential data consumption. Typical optimization techniques require an iterative gradient descent over multiple epochs of labeled data. While gradient methods are useful because they are defined at a mathematical level, they don't reveal much about the surface they're optimizing over. Our understanding of neural network behavior is undermined by this level of abstraction. As a consequence, training becomes a balancing act between model accuracy, availability of labeled data, and an ability to generalize unseen data. Recent works have focused on one-shot learning, a training paradigm that attempts to learn model parameters with only a single pass over the data [1 One shot-learning]. One-shot learning naturally combines with online learning models which encounter each training example exactly once before permanently discarding the data from training [https://arxiv.org/pdf/1605.06065.pdf].

\paragraph{}
While nearest neighbors is effectively a one-shot learning algorithm, model size grows linearly with amount of labeled data. Some recent network architectures incorporate a fixed-size augmented memory in addition to model parameters. Drawing inspiration from LSTM's forget gate, augmented memory is used to store additional state information that is explicitly tied to incoming data. Neural Turing Machines make use of so-called "working memory" to store 
Part of the appeal of one-shot learning is the ability to build a model in memory who's size does not grow with the amount of data. 

\paragraph{}
Gradient methods are appealing because they are able to find local minima on an unknown and potentially ill-behaved surface. This surface is defined by both the loss function and a neural network's feature encoding and decoding of the input data. Learning an effective encoding and decoding transformation partially contributes to gradient descent's long training time. Drawing insights from Holographic Reduced Representation, projecting data into a sufficiently high dimensions allows us to represent any transformation of features as a binding, unbinding, and superposition operator. Typical operators include