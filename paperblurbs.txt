Rumelhart and McCleland showed how neural networks could be used to learn the complicated nature of past-tense transformations of verbs. This means both learning a generalizable rule, as well as creating exceptions.

    The strategy used by Rumelhart and McCleland was to create a feature-based representation of words and then have the perceptron learning algorithm find a mapping between the two feature representations. The features were thought to carry the main clues as to the mapping and the rules to follow. They created a 'Wikelfeature' to describe each word, this was a phoneme-based representation of the word, and included abstract symbols to represent the beginning and end of a word. With such a feature representation, the relationships between common wikelfeatures of the present-tense word could be used to decide the rule and which other phonemes should be added and/or removed to create the past-tense word.

    \paragraph{}
    Goal 3 presents a fundamental challenge of performing a combinatoric search, and was a desired aspect of the original work but never realized. Because it is such a challenge to infer the word from the Wickelfeature representation, validation of the learning in goals 1/2 is difficult. To validate the performance of their original network, Rumelhart and McCleland examined the similarity of the abstract high-level Wickelfeature representations of the network's output to the correct words representation. Although this is indirect, comparison in the abstract feature space is still useful to understand the network's performance. Our predictions are decoded by a "resonance attractor", which applies expectation maximization to tune the prediction to represent a linear combination of letter vectors that spell out a correct past tense version of the input present tense verb. 



This algorithm can be seen to generalize to the test set, but it is challenging to interpret its performance purely in the abstract feature space. We see that the training set similarity increases beyond the normal value, which indicates that the network is over-emphasizing some aspects of the transformation. Essentially, this increase beyond $N$ in similarity is due to the same force that allows the network to generalize to novel verbs. As more verb pairs are added to the dictionary, then the common transformations start to stand out and get emphasized. The verbs in the training set all contribute to the similarity of the output, and many of them are contributing the same thing -- namely the regular rule of adding the trigram "ed." to create the past tense verbs. This explains the similarity going above $N$, as many verb-pairs are shouting the same correct feature. 




 We begin the search by initializing each of the guesses to the superposition of all the possible letters. This can also be done by initializing the guesses randomly, which is effectively just as good because the resonance step will keep you in the letter space. The better the guess is, then the more likely you will find the right answer. In the high-dimensional space, we can search in superposition, but the more guesses we hold in superposition the noisier the result. However, each clean-up iteration can make a cleaner guess -- by emphasizing the more likely alternatives, we search over fewer possibilities in superposition each iteration and thus get a less noisy answer. Then after enough iterations there is some probability that this decoding network will find a stable resonance, and the correct letter sequence can be decoded.

