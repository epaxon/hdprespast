{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to work with phonemes: http://www.nltk.org/_modules/nltk/corpus/reader/cmudict.html\n",
    "# from nltk.corpus import brown\n",
    "# from nltk.corpus import cmudict\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_examples(filepath):\n",
    "    examples = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                examples.append(word)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'B', u'AA1', u'R', u'B', u'AH0', u'L', u'Z']]\n",
      "[[u'B', u'AA1', u'R', u'B', u'IH0', u'K', u'Y', u'UW2']]\n",
      "[[u'B', u'AA1', u'R', u'B', u'IH0', u'K', u'Y', u'UW2', u'D']]\n",
      "[[u'B', u'AA1', u'R', u'B', u'IH0', u'K', u'Y', u'UW2', u'IH0', u'NG']]\n",
      "[[u'B', u'AA1', u'R', u'B', u'IH0', u'K', u'Y', u'UW2', u'Z']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from utils import random_idx\n",
    "# http://www.nltk.org/_modules/nltk/corpus/reader/cmudict.html\n",
    "# assign every phoneme to a feature vector\n",
    "k = 500\n",
    "N = 1000\n",
    "NUM_CLASSES = 2 # 0 for non-ed verb, 1 for ed verb\n",
    "phonemes = [\"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\", \"B\", \"CH\", \"D\", \"DH\", \"EH\", \"ER\", \"EY\", \"F\", \"G\", \"HH\", \"IH\",\n",
    "            \"IY\", \"JH\", \"K\", \"L\", \"M\", \"N\", \"NG\", \"OW\", \"OY\", \"P\", \"R\", \"S\", \"SH\", \"T\", \"TH\", \"UH\", \"UW\", \"V\", \"W\",\n",
    "            \"Y\", \"Z\", \"ZH\"\n",
    "           ]\n",
    "RI_phonemes = random_idx.generate_phoneme_id_vectors(N, k, phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first association is between ed verbs and their present tense\n",
    "nonedpast = read_examples(\"wickle_train/noned_past_example.txt\")\n",
    "edpast = read_examples(\"wickle_train/ed1000.txt\")\n",
    "past = nonedpast + edpast\n",
    "ytrain = np.ones(len(past))\n",
    "ytrain[:len(nonedpast)]  = 0\n",
    "# to be generated\n",
    "Xtrain = np.zeros((len(past), N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'W', u'AA1', u'Z'], [u'W', u'AH1', u'Z'], [u'W', u'AH0', u'Z'], [u'W', u'AO1', u'Z']]\n",
      "[[u'W', u'ER0'], [u'W', u'ER1']]\n",
      "[[u'B', u'IH1', u'N'], [u'B', u'AH0', u'N'], [u'B', u'IH0', u'N']]\n",
      "[[u'ER0', u'OW1', u'Z']]\n",
      "[[u'AH0', u'W', u'OW1', u'K']]\n",
      "[[u'W', u'AA1', u'Z'], [u'W', u'AH1', u'Z'], [u'W', u'AH0', u'Z'], [u'W', u'AO1', u'Z']]\n",
      "[[u'B', u'AO1', u'R']]\n",
      "[[u'B', u'AO1', u'R', u'N']]\n",
      "[[u'B', u'EY1', u'D']]\n",
      "[[u'B', u'IY1', u'T']]\n",
      "[[u'B', u'IY1', u'T', u'AH0', u'N']]\n",
      "[[u'B', u'IH0', u'K', u'EY1', u'M'], [u'B', u'IY0', u'K', u'EY1', u'M']]\n",
      "[[u'B', u'IH0', u'K', u'AH1', u'M']]\n",
      "[[u'B', u'IH0', u'G', u'AE1', u'N'], [u'B', u'IY0', u'G', u'AE1', u'N']]\n",
      "[[u'B', u'IY0', u'G', u'AO1', u'T']]\n",
      "[[u'B', u'IH0', u'G', u'AH1', u'N'], [u'B', u'EY1', u'G', u'AH0', u'N']]\n",
      "[[u'B', u'EH1', u'N', u'T']]\n",
      "[[u'B', u'EH1', u'T']]\n",
      "[[u'B', u'IH1', u'T']]\n",
      "[[u'B', u'IH1', u'T', u'AH0', u'N']]\n",
      "[[u'B', u'L', u'EH1', u'D']]\n",
      "[[u'B', u'L', u'UW1']]\n",
      "[[u'B', u'L', u'OW1', u'N']]\n",
      "[[u'B', u'OW1', u'D']]\n",
      "[[u'B', u'R', u'EH1', u'D']]\n",
      "[[u'B', u'R', u'OW1', u'K']]\n",
      "[[u'B', u'R', u'OW1', u'K', u'AH0', u'N']]\n",
      "[[u'B', u'R', u'AO1', u'T']]\n",
      "[[u'B', u'AW1', u'N', u'D']]\n",
      "[[u'B', u'IH1', u'L', u'T']]\n",
      "[[u'B', u'ER1', u'N', u'T']]\n",
      "[[u'B', u'ER1', u'S', u'T']]\n",
      "[[u'B', u'AA1', u'T'], [u'B', u'AO1', u'T']]\n",
      "[[u'K', u'AE1', u'S', u'T']]\n",
      "[[u'K', u'AA1', u'T'], [u'K', u'AO1', u'T']]\n",
      "[[u'CH', u'OW1', u'Z']]\n",
      "[[u'CH', u'OW1', u'Z', u'AH0', u'N']]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'clove'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a1a07a07aa9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0marpabet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmudict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marpabet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# make the test set the brown corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'clove'"
     ]
    }
   ],
   "source": [
    "# encode verbs as phonemes, then \n",
    "arpabet = nltk.corpus.cmudict.dict()\n",
    "for word in (past):\n",
    "    # need to fix key errors.\n",
    "    print(arpabet[word])\n",
    "    \n",
    "# make the test set the brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing:\n",
    "DONE: phonetic encoding of verbs based on cmu dictionary\n",
    "TO DO: categorize verbs based on their frequency (just low, medium, or high)\n",
    "\n",
    "Pentti:\n",
    "These are my notes on learning the past tense of\n",
    "English verbs (\"Chapter18.pdf\" of Rumelhart &\n",
    "McClelland 1986) from the meeting on Friday.  We'll\n",
    "learn their model and the Perceptron Learning Rule\n",
    "by reproducing the work in sections \"Learning\" and\n",
    "\"Learning Regular and Exceptional Patterns in a Pattern\n",
    "Associator\" (pp. 225-233).  It's a simple exercise with\n",
    "artificial data.  Here's what I'd like you to do:\n",
    "\n",
    "1. First see how Figure 3 corresponds to having learned\n",
    "   one of the association (one input-output pair).\n",
    "\n",
    "2. Then start with an 8 x 8 matrix of 0s and train it\n",
    "   for that same association using the perceptron-\n",
    "   learning rule, to get a matrix that looks like Table\n",
    "   1A.  Because training is stochastic, the numbers you\n",
    "   get will not be exactly the same.\n",
    "\n",
    "3. Do the same with the other association (Table 2B).\n",
    "\n",
    "4. Then reproduce Table 2C.  Now we are learning two\n",
    "   associations.\n",
    "\n",
    "5. Figure out what they mean by \"Rule of 78\" and see if\n",
    "   you can reproduce the equivalent of Table 2D.\n",
    "\n",
    "6. Finally, see if you can reproduce all four stages\n",
    "   shown in Table 4.\n",
    "\n",
    "I'd like to see all 8 matrices you get, all that\n",
    "correspond to those sown in Tables 2 and 4.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]: -5.44855538562 -> 0\n",
      "[0 1]: -5.18683850822 -> 0\n",
      "[1 0]: -5.16465588571 -> 0\n",
      "[1 1]: -4.90293900831 -> 0\n"
     ]
    }
   ],
   "source": [
    "# https://blog.dbrgn.ch/2013/3/26/perceptrons-in-python/\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from numpy import array, dot, random\n",
    "\n",
    "unit_step = lambda x: 0 if x < 0 else 1\n",
    "\n",
    "training_data = [\n",
    "    (array([0,0,1]), 0),\n",
    "    (array([0,1,1]), 1),\n",
    "    (array([1,0,1]), 1),\n",
    "    (array([1,1,1]), 1),\n",
    "]\n",
    "\n",
    "w = random.rand(3)\n",
    "errors = []\n",
    "eta = 0.2\n",
    "n = 100\n",
    "\n",
    "for i in xrange(n):\n",
    "    x, y = choice(training_data)\n",
    "    yHat = dot(w, x)\n",
    "    error = y - unit_step(result)\n",
    "    errors.append(error)\n",
    "    w += eta * error * x\n",
    "\n",
    "for x, _ in training_data:\n",
    "    result = dot(x, w)\n",
    "    print(\"{}: {} -> {}\".format(x[:2], result, unit_step(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html\n",
    "# does not guarantee large/good margin.\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, eta=0.01, epochs=50):\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self, X, y):\n",
    "\n",
    "        self.w_ = np.zeros(1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_[1:] +=  update * xi\n",
    "                self.w_[0] +=  update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
