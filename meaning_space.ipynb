{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "from stop_words import get_stop_words\n",
    "#from scipy import sparse\n",
    "from scipy import spatial\n",
    "import gensim\n",
    "import utils\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from utils import random_idx\n",
    "from utils import utils\n",
    "from utils import lang_vectors_utils as lvu\n",
    "from sklearn import manifold, datasets\n",
    "#import lang_vectors_utils\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "num_topics = 50\n",
    "passes = 20\n",
    "#topn = 10\n",
    "k = 5000\n",
    "N = 10000\n",
    "# cluster_sizes is mapping to n-gram size\n",
    "# cluster_sz in random_idx referring to specific element (int) in cluster_sizes, array\n",
    "cluster_sizes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "ordered = 1\n",
    "#assuming this is the alphabet bc of precedent in generate_text.py\n",
    "#alph = 'abc' \n",
    "alphabet = string.lowercase + ' '\n",
    "RI_letters = random_idx.generate_letter_id_vectors(N, k, alphabet)\n",
    "# number of stacked syntax vectors per meaning matrix\n",
    "meaning_granularities = [5,10,100,200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_doc_set(path, files):\n",
    "    doc_set = []\n",
    "    for filename in files:\n",
    "        f = open(path + filename, \"r\")\n",
    "        doc_set.append(f.read())\n",
    "        f.close()\n",
    "    return doc_set\n",
    "\n",
    "\n",
    "def tokenize(doc_set):\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def meaning_matrix(ldamodel, topicid, topn, dictionary):\n",
    "    \" NO \"\n",
    "    # token 2 id dictionary\n",
    "    # print dictionary.token2id\n",
    "    matrix = np.zeros((N,N))\n",
    "    id2token = dictionary.id2token\n",
    "    topic_terms = []\n",
    "\n",
    "    for tup in ldamodel.get_topic_terms(topicid, topn):\n",
    "        topic_terms.append(str(id2token[tup[0]]))\n",
    "\n",
    "    for i in range(0,topn):\n",
    "        term_vector = random_idx.id_vector(N, topic_terms[i], alphabet, RI_letters, ordered)\n",
    "        matrix[i] = term_vector\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def meaning_matrices(ldamodel, num_topics, topn, dictionary):\n",
    "    \" NO \"\n",
    "    matrices = np.zeros((num_topics,N,N))\n",
    "    for topicid in range(0,num_topics):\n",
    "        matrices[topicid] = create_meaning_matrix(ldamodel, topicid, topn, dictionary)\n",
    "    return matrices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_dictionary(dictionary):\n",
    "    vectors = {}\n",
    "    for token_id, token in dictionary.items():\n",
    "            vectors[token_id] = random_idx.id_vector(N, token, alphabet, RI_letters)\n",
    "    return vectors\n",
    "\n",
    "def meaning_space(ldamodel, num_topics, dictionary, vectorized_dictionary):\n",
    "    \"\"\"\n",
    "    number of granularities meaning space x number of topics x granularity x N\n",
    "    index in numpy array on axis \"number of tokens\" equivalent to token id\n",
    "    \"\"\"\n",
    "    # for token ids\n",
    "    num_tokens = len(dictionary.keys())\n",
    "    matrices = []\n",
    "    for gran_i in range(len(meaning_granularities)):\n",
    "        matrix = np.zeros((num_topics, meaning_granularities[gran_i], N))\n",
    "        for topicid in range(num_topics):\n",
    "            terms = ldamodel.get_topic_terms(topicid, meaning_granularities[gran_i])\n",
    "            for i in range(len(terms)):\n",
    "                token_id = terms[i][0]\n",
    "                matrix[topicid][i] = vectorized_dictionary[token_id]\n",
    "        matrices.append(matrix)\n",
    "    \n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_meaning(test_files, tokenized_test_documents, dictionary, meaninged_space, meaning_granularities):\n",
    "    # you can't test a meaning matrix...try graphing first\n",
    "    \"\"\"\n",
    "    meaninged_space dimensions: number of granularities meaning space x number of topics x granularity x N\n",
    "    prediction_vector = argmax(np.dot(test_token_vector, meaning matrix i))\n",
    "    graph prediction vector in matplotlib\n",
    "    N dimensions lmao. how to reduce dimensionality\n",
    "    \"\"\"\n",
    "    # each vector maps to the test_token. dots graphs but mappings can printed\n",
    "    token_to_id = dictionary.token2id\n",
    "    # num test_files x number of granularities x number of topics\n",
    "    # so each topic gets a list of test_token_vectors\n",
    "    topic_sets = []\n",
    "    for i in range(len(test_files)):\n",
    "        topic_sets.append([])\n",
    "        for j in range(len(meaning_granularities)):\n",
    "            topic_sets[i].append([])\n",
    "            for k in range(num_topics):\n",
    "                topic_sets[i][j].append([])\n",
    "\n",
    "    print len(topic_sets)\n",
    "    print len(topic_sets[0])\n",
    "    print len(topic_sets[0][0])\n",
    "    \n",
    "    # dictionary of lists of tokens specific to that topic\n",
    "    topic_to_tokens = {}\n",
    "    for topicid in range(num_topics):\n",
    "        topic_to_tokens[topicid] = []\n",
    "        \n",
    "    for i in range(len(test_files)):\n",
    "        test_tokens = tokenized_test_documents[i]\n",
    "        for gran_i in range(len(meaning_granularities)):\n",
    "            for test_token in test_tokens:\n",
    "                vectorized_test_token = random_idx.id_vector(N, test_token, alphabet, RI_letters)\n",
    "                # test_token vector will dot product with every matrix \n",
    "                # and will match up to a vector in that matrix that would make a high dot product\n",
    "                # shapes (50,5,10000) and (1,10000) not aligned: 10000 (dim 2) != 1 (dim 0)\n",
    "                prediction_matrix = np.dot(meaninged_space[gran_i], vectorized_test_token[0])\n",
    "                topic_similarity = np.zeros(prediction_matrix.shape[0])\n",
    "                for topicid in range(prediction_matrix.shape[0]):\n",
    "                    topic_similarity[topicid] = sum(prediction_matrix[topicid])\n",
    "                matching_topicid = np.argmax(topic_similarity)\n",
    "                topic_sets[i][gran_i][matching_topicid].append(vectorized_test_token)\n",
    "                topic_to_tokens[matching_topicid].append(test_token)\n",
    "    \n",
    "    return topic_sets, topic_to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_3d():\n",
    "    # each vector maps to the test_token. dots graphs but mappings can printed\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    xs =[1,2,3,4,5,6,7,8,9,10]\n",
    "    ys =[5,6,2,3,13,4,1,2,4,8]\n",
    "    zs =[2,3,3,3,5,7,9,11,9,10]\n",
    "\n",
    "    xt =[-1,-2,-3,-4,-5,-6,-7,8,-9,-10]\n",
    "    yt =[-5,-6,-2,-3,-13,-4,-1,2,-4,-8]\n",
    "    zt =[-2,-3,-3,-3,-5,-7,9,-11,-9,-10]\n",
    "\n",
    "    ax.scatter(xs, ys, zs, c='r', marker='o')\n",
    "    ax.scatter(xt, yt, zt, c='b', marker='^')\n",
    "\n",
    "    ax.set_xlabel('X Label')\n",
    "    ax.set_ylabel('Y Label')\n",
    "    ax.set_zlabel('Z Label')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "topic sets is: len(test_files) x len(granularities) x num_topics where each element is a list of vectors belonging to that topic\n",
    "these are the points. \n",
    "graph these points along with the training set. \n",
    "meaninged_space = number of granularities x number of topics x granularity x N\n",
    "where the granularity dimension contains the rows of points.\n",
    "and if you wanna see some of the words from the training set in each topic:\n",
    "terms = ldamodel.get_topic_terms(topicid, meaning_granularities[gran_i])\n",
    "\"\"\"\n",
    "def graph(test_files, meaning_granularities, num_topics, topic_sets, topic_to_tokens, meaninged_space):\n",
    "    n_components = 2 \n",
    "    n_neighbors = 10\n",
    "    method = 'hessian'\n",
    "    # actual meaninged_space dimensions: (2, 50, 5, 10000)\n",
    "    # dimensions wrong. need to fix. other than that, graphing is good.\n",
    "    #a = '../preprocessed_texts/english/with_spaces/alice_in_wonderland.txt'\n",
    "\n",
    "    #one_hot_encoding = random_idx.generate_letter_id_vectors(N, k)\n",
    "    for i in range(len(test_files)):\n",
    "        for j in range(len(meaning_granularities)):\n",
    "            for k in range(num_topics):\n",
    "                print meaninged_space[i][j][k].shape\n",
    "                lst = meaninged_space[i][j][k].tolist()\n",
    "                print len(lst)\n",
    "                tup = tuple(lst)\n",
    "\n",
    "                big_matrix = np.vstack(lst)\n",
    "                big_matrix = big_matrix[0:1000]\n",
    "                print big_matrix.shape\n",
    "\n",
    "                print \"compressing data\"\n",
    "\n",
    "                fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "                tsne = manifold.TSNE(n_components=n_components, init='pca', random_state=0)\n",
    "                Y = tsne.fit_transform(big_matrix)\n",
    "\n",
    "                plt.scatter(Y[:, 0], Y[:, 1], cmap=plt.cm.Spectral)\n",
    "                plt.show()\n",
    "                se = manifold.SpectralEmbedding(n_components=n_components,n_neighbors=n_neighbors)\n",
    "                Y = se.fit_transform(big_matrix)\n",
    "                plt.scatter(Y[:, 0], Y[:, 1], cmap=plt.cm.Spectral)\n",
    "                plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "raw_path = \"raw_texts/texts_english/\"\n",
    "preprocessed_path = \"preprocessed_texts/english/\"\n",
    "training_preprocessed_path = \"preprocessed_texts/english/with_spaces/\"\n",
    "\n",
    "training_files = [\"a_christmas_carol.txt\", \"alice_in_wonderland.txt\"]\n",
    "# this is for testing accuracy against the \n",
    "# actual stream that will be the test input\n",
    "test_files = [\"hamlet_english.txt\", \"percy_addleshaw.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_doc_set = create_doc_set(training_preprocessed_path, training_files)\n",
    "test_doc_set = create_doc_set(preprocessed_path, test_files)\n",
    "\n",
    "tokenized_training_documents = tokenize(training_doc_set)\n",
    "tokenized_test_documents = tokenize(test_doc_set)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(tokenized_training_documents)\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_training_documents]\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorized_dictionary = vectorize_dictionary(dictionary)\n",
    "fwrite = open(\"meaning_vectorized_dictionary\", \"w\")\n",
    "pickle.dump(vectorized_dictionary, fwrite)\n",
    "fwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# meaning matrix\n",
    "fread = open(\"meaning_vectorized_dictionary\", \"r\")\n",
    "vectorized_dictionary = pickle.load(fread)\n",
    "fread.close()\n",
    "meaninged_space = meaning_space(ldamodel, num_topics, dictionary, vectorized_dictionary)\n",
    "#print meaninged_space\n",
    "fwrite = open(\"meaninged_space\", \"w\")\n",
    "pickle.dump(meaninged_space, fwrite)\n",
    "fwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# test results\n",
    "fread = open(\"meaning_vectorized_dictionary\", \"r\")\n",
    "fread1 = open(\"meaninged_space\", \"r\")\n",
    "vectorized_dictionary = pickle.load(fread)\n",
    "meaninged_space = pickle.load(fread1)\n",
    "fread.close()\n",
    "fread1.close()\n",
    "\n",
    "topic_sets, topic_to_tokens = validate_meaning(test_files, tokenized_test_documents, dictionary, meaninged_space, meaning_granularities)\n",
    "fwrite = open(\"topic_sets\", \"w\")\n",
    "fwrite1 = open(\"topic_to_tokens\", \"w\")\n",
    "pickle.dump(topic_sets, fwrite)\n",
    "pickle.dump(topic_to_tokens, fwrite1)\n",
    "fwrite.close()\n",
    "fwrite1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 5, 10000)\n",
      "(10000,)\n",
      "10000\n",
      "(1000, 1)\n",
      "compressing data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-6ef682d1cd93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeaning_granularities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_to_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeaninged_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-656ebccd16c1>\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(test_files, meaning_granularities, num_topics, topic_sets, topic_to_tokens, meaninged_space)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanifold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpectral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quinntran/anaconda/lib/python2.7/site-packages/sklearn/manifold/t_sne.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \"\"\"\n\u001b[0;32m--> 866\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quinntran/anaconda/lib/python2.7/site-packages/sklearn/manifold/t_sne.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    775\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m     def _tsne(self, P, degrees_of_freedom, n_samples, random_state,\n",
      "\u001b[0;32m/Users/quinntran/anaconda/lib/python2.7/site-packages/sklearn/manifold/t_sne.pyc\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, random_state, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;31m# Early exaggeration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_exaggeration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopt_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m         \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_iter'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quinntran/anaconda/lib/python2.7/site-packages/sklearn/manifold/t_sne.pyc\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, objective_error, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, min_error_diff, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mnew_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/quinntran/anaconda/lib/python2.7/site-packages/sklearn/manifold/t_sne.pyc\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, neighbors, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \"\"\"\n\u001b[1;32m    278\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mX_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0mneighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103f05f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fread = open(\"meaning_vectorized_dictionary\", \"r\")\n",
    "fread1 = open(\"meaninged_space\", \"r\")\n",
    "fread2 = open(\"topic_sets\", \"r\")\n",
    "fread3 = open(\"topic_to_tokens\", \"r\")\n",
    "vectorized_dictionary = pickle.load(fread)\n",
    "meaninged_space = pickle.load(fread1)\n",
    "topic_sets = pickle.load(fread2)\n",
    "topic_to_tokens = pickle.load(fread3)\n",
    "fread.close()\n",
    "fread1.close()\n",
    "fread2.close()\n",
    "fread3.close()\n",
    "graph(test_files, meaning_granularities, num_topics, topic_sets, topic_to_tokens, meaninged_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
